{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Installing and importing libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j-sgUbzBXPZK","outputId":"7871ebf9-f461-4ef3-ff67-4355d1ab1de3","trusted":true},"outputs":[],"source":["!pip install -q transformers seqeval[gpu]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEnlUbgm8z3B","trusted":true},"outputs":[],"source":["import os\n","import time\n","import json\n","import torch\n","import random\n","import zipfile\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from torch import cuda\n","from tabulate import tabulate\n","from sklearn.metrics import accuracy_score\n","from torch.utils.data import Dataset, DataLoader\n","from seqeval.metrics import classification_report\n","from transformers import RobertaTokenizerFast, RobertaForTokenClassification"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sm1krxJtKxpx","outputId":"20ef661a-afa6-47ae-88b3-4a9781299a53","tags":[],"trusted":true},"outputs":[],"source":["# Set the default device to GPU\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"ahwMsmyG5ZPE"},"source":["## Setup parameters and functions"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create list of categories\n","category_tags = [\"APT\", \"SECTEAM\", \"IDTY\", \"OS\", \"EMAIL\", \"LOC\", \"TIME\", \"IP\", \"DOM\", \"URL\", \"PROT\", \n","         \"FILE\", \"TOOL\", \"MD5\", \"SHA1\", \"SHA2\", \"MAL\", \"ENCR\", \"VULNAME\", \"VULID\", \"ACT\"]\n","limits = [\"B\", \"I\"]  # Annotation scheme\n","entity_tags = [\"O\"] + [limit + \"-\" + tag for tag in category_tags for limit in limits]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create dictionaries\n","labels_to_ids = {label: str(i) for i,label in enumerate(entity_tags)}  # Maps individual tags to indices\n","ids_to_labels = {str(i): label for i,label in enumerate(entity_tags)}  # Maps indices to individual tags"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["SEED_VAL = 42\n","MAX_LEN = 128\n","BATCH_SIZE = 16\n","EPOCHS = 15\n","LEARNING_RATE = 2e-05\n","MAX_GRAD_NORM = 10\n","model_type = \"s2w-ai/DarkBERT\"\n","access_token = \"\"\n","uncased = False\n","\n","tokenizer = RobertaTokenizerFast.from_pretrained(model_type, add_prefix_space=True, token=access_token)"]},{"cell_type":"markdown","metadata":{},"source":["#### Dataset loading"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the function to create df\n","def create_dataframe_from_file(filename, uncased=False):\n","    with open(filename, 'r', encoding='utf-8') as file:\n","            data = json.load(file)\n","            \n","    entries = []\n","    for entry in data:\n","        if entry['sentence'] == \"\":\n","            continue\n","        if uncased:\n","            entries.append({'sentence': entry['sentence'].lower(), 'word_labels': entry['tags']})\n","        else:  # Cased\n","            entries.append({'sentence': entry['sentence'], 'word_labels': entry['tags']})\n","            \n","\n","    # Create a DataFrame from the dictionary\n","    df = pd.DataFrame(entries)\n","    return df"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def check_test_tags(data):\n","    # Initialize a dictionary to store the count of each type\n","    type_counts = {tag: 0 for tag in category_tags}\n","\n","    # Iterate through each dictionary in the list\n","    for i, entry in data.iterrows():\n","        tags_list = entry[\"word_labels\"].split(',')\n","        for tag in tags_list:\n","            if tag != 'O':\n","                # Split the tag into limit and entity\n","                limit, entity = tag.split('-')\n","                # Increment the count for the corresponding type\n","                type_counts[entity] += 1\n","\n","    # Convert type_counts to two lists for tabulate (horizontal)\n","    table_data = [[\"Entity\"] + list(type_counts.keys()), [\"Count\"] + list(type_counts.values())]\n","\n","    # Print the table horizontally\n","    table = tabulate(table_data, tablefmt=\"plain\")\n","    print(table)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define labels at wordpiece-level --> handle this by only train the model on the tag labels for the first word piece token of a word\n","class dataset(Dataset):  # Transforms examples of a dataframe to PyTorch tensors\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __getitem__(self, index):\n","        # step 1: get the sentence and word labels\n","        sentence = self.data.sentence[index].strip().split()\n","        word_labels = self.data.word_labels[index].split(\",\")\n","\n","        # step 2: use tokenizer to encode sentence (includes padding/truncation up to max length)\n","        # RobertaTokenizerFast provides a handy \"return_offsets_mapping\" functionality for individual tokens\n","        encoding = self.tokenizer(sentence,\n","                             is_split_into_words=True,\n","                             return_offsets_mapping=True,\n","                             padding='max_length',\n","                             truncation=True,\n","                             max_length=self.max_len)\n","    \n","        # step 3: create token labels only for first word pieces of each tokenized word\n","        labels = [labels_to_ids[label] for label in word_labels]\n","        # code based on https://huggingface.co/transformers/custom_datasets.html#tok-ner\n","        # create an empty array of -100 of length max_length\n","        encoded_labels = np.ones(len(encoding[\"offset_mapping\"]), dtype=int) * -100  # 100 is the default ignore_index of PyTorch's CrossEntropyLoss\n","        \n","        tokenized_sentence = [tokenizer.tokenize(word) for word in sentence]\n","        tokens_len_list = []\n","        for toks in tokenized_sentence:\n","            for i in range(1, len(toks)+1):\n","                tokens_len_list.append(i)\n","        len_toks = len(tokens_len_list)        \n","        \n","        i=0\n","        for idx, mapping in enumerate(encoding[\"offset_mapping\"]):\n","            if not(mapping[0] == 0 and mapping[1] == 0):\n","                # overwrite label\n","                encoded_labels[idx] = labels[i]\n","                if idx >= len_toks or tokens_len_list[idx] == 1:  # Only if the next mapping/token belongs to a new word\n","                    i+=1\n","\n","        # step 4: turn everything into PyTorch tensors\n","        item = {key: torch.as_tensor(val) for key, val in encoding.items()}\n","        item['labels'] = torch.as_tensor(encoded_labels)\n","\n","        return item\n","\n","    def __len__(self):\n","        return self.len"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_dataset_from(directory, uncased=False):\n","    df_train = create_dataframe_from_file(os.path.join(directory, 'Train.json'), uncased=uncased)\n","    df_val = create_dataframe_from_file(os.path.join(directory, 'Val.json'), uncased=uncased)\n","    df_test = create_dataframe_from_file(os.path.join(directory, 'Test.json'), uncased=uncased)\n","    \n","    print(\"TRAIN Dataset: {}\".format(df_train.shape))\n","    print(\"VAL Dataset: {}\".format(df_val.shape))\n","    print(\"TEST Dataset: {}\".format(df_test.shape))\n","    check_test_tags(df_test)\n","    \n","    training_set = dataset(df_train, tokenizer, MAX_LEN)\n","    val_set = dataset(df_val, tokenizer, MAX_LEN)\n","    testing_set = dataset(df_test, tokenizer, MAX_LEN)\n","    \n","    # Define the PyTorch dataloaders\n","    train_params = {'batch_size': BATCH_SIZE,\n","                    'shuffle': True,\n","                    'num_workers': 0\n","                    }\n","\n","    val_params = {'batch_size': BATCH_SIZE,\n","                    'shuffle': False,\n","                    'num_workers': 0\n","                    }\n","\n","    test_params = {'batch_size': BATCH_SIZE,\n","                    'shuffle': False,\n","                    'num_workers': 0\n","                    }\n","    \n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","    testing_loader = DataLoader(testing_set, **test_params)\n","    \n","    return training_loader, val_loader, testing_loader"]},{"cell_type":"markdown","metadata":{},"source":["#### Training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Helping function for time\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Use plot styling from seaborn\n","def plot_stats(df_stats):\n","    sns.set(style='darkgrid')\n","\n","    # Increase the plot size and font size\n","    sns.set(font_scale=1.5)\n","    plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","    # Plot the learning curve\n","    plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","    plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","    # Label the plot\n","    plt.title(\"Training & Validation Loss\")\n","    plt.xlabel(\"Epoch\")\n","    plt.ylabel(\"Loss\")\n","    plt.legend()\n","    plt.xticks([1, 2, 3, 4])\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Summary of the training process\n","def display_df_stats(training_stats):\n","    pd.reset_option('^display.')\n","    pd.set_option('display.precision', 2)\n","\n","    # Create a DataFrame from the training statistics\n","    df_stats = pd.DataFrame(data=training_stats)\n","\n","    # Use the 'epoch' as the row index\n","    df_stats = df_stats.set_index('epoch')\n","\n","    # Display the table\n","    display(df_stats)\n","    \n","    plot_stats(df_stats)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define training function \n","def start_training(model_name, training_loader, val_loader): \n","    model = RobertaForTokenClassification.from_pretrained(model_type, num_labels=len(labels_to_ids), token=access_token)\n","    model.to(device)\n","\n","    # Define the optimizer\n","    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n","\n","    random.seed(SEED_VAL)\n","    np.random.seed(SEED_VAL)\n","    torch.manual_seed(SEED_VAL)\n","    torch.cuda.manual_seed_all(SEED_VAL)\n","\n","    # We'll store a number of quantities such as training and validation loss,\n","    # validation accuracy, and timings\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run\n","    total_t0 = time.time()\n","\n","    # For each epoch...\n","    for epoch_i in range(0, EPOCHS):\n","        # ========================================\n","        #               Training\n","        # ========================================\n","\n","        # Perform one full pass over the training set\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, EPOCHS))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes\n","        t0 = time.time()\n","\n","        # Reset for this epoch    \n","        tr_loss, tr_accuracy = 0, 0\n","        nb_tr_examples, nb_tr_steps = 0, 0\n","        tr_preds, tr_labels = [], []\n","\n","        # Put the model into training mode\n","        model.train()\n","\n","        # For each batch of training data...\n","        for idx, batch in enumerate(training_loader):\n","\n","            # Progress update every 40 batches\n","            if idx % 40 == 0 and not idx == 0:\n","                # Calculate elapsed time in minutes\n","                elapsed = format_time(time.time() - t0)\n","\n","                # Report progress\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(idx, len(training_loader), elapsed))\n","\n","            # Unpack this training batch from the dataloader    \n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","\n","            # clear any previously calculated gradients \n","            model.zero_grad()\n","\n","            # Perform a forward pass\n","            result = model(input_ids=ids, attention_mask=mask, labels=labels)#, return_dict=True)\n","            loss = result.loss\n","            tr_loss += loss\n","            tr_logits = result.logits\n","\n","            nb_tr_steps += 1\n","            nb_tr_examples += labels.size(0)\n","\n","            # compute training accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","\n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","            #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","            tr_labels.extend(labels)\n","            tr_preds.extend(predictions)\n","\n","            tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            tr_accuracy += tmp_tr_accuracy\n","\n","            # gradient clipping\n","            torch.nn.utils.clip_grad_norm_(\n","                parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","            )\n","\n","            # backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","\n","        # Calculate the average loss over all of the batches\n","        avg_train_loss = tr_loss / len(training_loader)\n","\n","        # Calculate the average accuracy over all of the batches\n","        tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","        # Measure how long this epoch took\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Average training accuracy: {0:.2f}\".format(tr_accuracy))\n","\n","        print(\"  Training epoch took: {:}\".format(training_time))\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure the performance on\n","        # the validation set\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation\n","        model.eval()\n","\n","        # Tracking variables\n","        eval_loss, eval_accuracy = 0, 0\n","        nb_eval_examples, nb_eval_steps = 0, 0\n","        eval_preds, eval_labels = [], []\n","        best_eval_loss = 1\n","\n","        # Evaluate data for one epoch\n","        for batch in tqdm(val_loader, desc=\"Validation\"):\n","\n","            # Unpack this training batch from dataloader\n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","\n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training)\n","            with torch.no_grad():\n","                # Forward pass, calculate logit predictions.\n","                result = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=True)\n","\n","            loss = result.loss\n","            eval_logits = result.logits\n","\n","            # Accumulate the validation loss\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","\n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","\n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","\n","            labels_list = [ids_to_labels[str(id.item())] for id in labels] \n","            predictions_list = [ids_to_labels[str(id.item())] for id in predictions] \n","\n","            eval_labels.append(labels_list)\n","            eval_preds.append(predictions_list)\n","\n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","\n","        # Report the final accuracy for this validation run\n","        avg_val_accuracy = eval_accuracy / len(val_loader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches\n","        avg_val_loss = eval_loss / len(val_loader)\n","\n","        # Measure how long the validation run took\n","        validation_time = format_time(time.time() - t0)\n","\n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        # Save best model\n","        if avg_val_loss < best_eval_loss:\n","            torch.save(model, model_name)\n","            best_eval_loss = avg_val_loss\n","\n","        # Record all statistics from this epoch\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss.item(),\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    \n","    display_df_stats(training_stats)"]},{"cell_type":"markdown","metadata":{"id":"73OzU7oXRxR8"},"source":["#### Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cB9MR3KcWXUs","outputId":"527586e3-eb7f-4b6d-93f0-2f932e1ab657","tags":[],"trusted":true},"outputs":[],"source":["# Define testing function\n","def start_testing(model_name, testing_loader):\n","    # Load best model\n","    model = torch.load(model_name)\n","    \n","    # put model in evaluation mode\n","    model.eval()\n","\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(testing_loader, desc=\"Testing\"):\n","\n","            ids = batch['input_ids'].to(device, dtype = torch.long)\n","            mask = batch['attention_mask'].to(device, dtype = torch.long)\n","            labels = batch['labels'].to(device, dtype = torch.long)\n","            \n","            result = model(input_ids=ids, attention_mask=mask, labels=labels, return_dict=True)\n","            loss = result.loss\n","            eval_loss += loss\n","            eval_logits = result.logits\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","\n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","\n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            labels_list = [ids_to_labels[str(id.item())] for id in labels] \n","            predictions_list = [ids_to_labels[str(id.item())] for id in predictions] \n","\n","            #eval_labels.extend(labels)\n","            #eval_preds.extend(predictions)\n","            \n","            eval_labels.append(labels_list)\n","            eval_preds.append(predictions_list)\n","\n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","            \n","            \n","    #labels = [ids_to_labels[str(id.item())] for id in eval_labels] \n","    #predictions = [ids_to_labels[str(id.item())] for id in eval_preds] \n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    print(f\"Testing Loss: {eval_loss}\")\n","    print(f\"Testing Accuracy: {eval_accuracy}\")\n","\n","    print(classification_report(eval_labels, eval_preds))"]},{"cell_type":"markdown","metadata":{"id":"sqDklprSqB5d"},"source":["## Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["main_dir = 'REP'\n","folder = 'NO3_REP'\n","model_name = 'REP_NER_model'"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(f'RUNNING {folder}')\n","directory = os.path.join('/home/anon/input/final-ner-datasets/Final_NER_datasets', main_dir, folder)\n","train_l, val_l, test_l = load_dataset_from(directory, uncased=uncased)  \n","model_name = 'REP_NER_model'\n","start_training(model_name, train_l, val_l)\n","start_testing(model_name, test_l)"]},{"cell_type":"markdown","metadata":{},"source":["## Saving & Loading Fine-Tuned Model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = torch.load(model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDZtSsKKntuI","tags":[],"trusted":true},"outputs":[],"source":["output_dir = \"/home/anon/working/REP_NER_model_folder\"\n","\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model = torch.load(model_name)\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Create a zip file\n","output_dir = '/home/anon/working/REP_NER_model_folder'\n","zip_path = '/home/anon/working/REP_NER_model.zip'\n","\n","with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","    for root, _, files in os.walk(output_dir):\n","        for file in files:\n","            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\"\"\"# Load trained model and vocabulary \n","zip_path = '/home/anon/working/REP_NER_model.zip'\n","output_dir = '/home/anon/working/REP_NER_model'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(output_dir)\n","\n","# Load the tokenizer\n","tokenizer = RobertaTokenizerFast.from_pretrained(output_dir)\n","\n","# Load the fine-tuned model\n","model = RobertaForTokenClassification.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)\"\"\""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
