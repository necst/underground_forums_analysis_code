{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:25.340362Z","iopub.status.busy":"2023-07-11T22:29:25.339319Z","iopub.status.idle":"2023-07-11T22:29:42.001357Z","shell.execute_reply":"2023-07-11T22:29:42.000156Z","shell.execute_reply.started":"2023-07-11T22:29:25.340313Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import time\n","import json\n","import torch\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{},"source":["### Setup for GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:42.004308Z","iopub.status.busy":"2023-07-11T22:29:42.003928Z","iopub.status.idle":"2023-07-11T22:29:42.012664Z","shell.execute_reply":"2023-07-11T22:29:42.011648Z","shell.execute_reply.started":"2023-07-11T22:29:42.004277Z"},"trusted":true},"outputs":[],"source":["# To identify and specify the GPU\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():\n","\n","    # Tell PyTorch to use the GPU\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{},"source":["### Setup the pathes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:42.020851Z","iopub.status.busy":"2023-07-11T22:29:42.020170Z","iopub.status.idle":"2023-07-11T22:29:42.029878Z","shell.execute_reply":"2023-07-11T22:29:42.028493Z","shell.execute_reply.started":"2023-07-11T22:29:42.020817Z"},"trusted":true},"outputs":[],"source":["model_input_dir = '../input/classifier-bert'  # Directory with the fine-tuned BERT model\n","\n","df_input_dir = '/home/anon/input/chunks-other-entries-not-labeled-processed'  # Directory with the posts to label\n","json_output_dir = '/home/anon/working/outputs'  # Direcotry with the final predicted posts\n","if not os.path.exists(json_output_dir):\n","    os.makedirs(json_output_dir)\n","\n","intermediate_predictions_dir = '/home/anon/working/intermediate_predictions'  # Directory with intermediate predictions\n","if not os.path.exists(intermediate_predictions_dir):\n","    os.makedirs(intermediate_predictions_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:42.032143Z","iopub.status.busy":"2023-07-11T22:29:42.031739Z","iopub.status.idle":"2023-07-11T22:29:47.408573Z","shell.execute_reply":"2023-07-11T22:29:47.406996Z","shell.execute_reply.started":"2023-07-11T22:29:42.032111Z"},"trusted":true},"outputs":[],"source":["# Load the tokenizer\n","tokenizer = BertTokenizer.from_pretrained(model_input_dir)\n","\n","# Load the fine-tuned model\n","model = BertForSequenceClassification.from_pretrained(model_input_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.410453Z","iopub.status.busy":"2023-07-11T22:29:47.410093Z","iopub.status.idle":"2023-07-11T22:29:47.416163Z","shell.execute_reply":"2023-07-11T22:29:47.414854Z","shell.execute_reply.started":"2023-07-11T22:29:47.410422Z"},"trusted":true},"outputs":[],"source":["# Define the maximum sequence length for each window - as in the training phase\n","window_length = 512\n","\n","# The DataLoader needs to know the batch size\n","batch_size = 64  "]},{"cell_type":"markdown","metadata":{},"source":["### Functions definition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.418599Z","iopub.status.busy":"2023-07-11T22:29:47.418136Z","iopub.status.idle":"2023-07-11T22:29:47.432135Z","shell.execute_reply":"2023-07-11T22:29:47.430450Z","shell.execute_reply.started":"2023-07-11T22:29:47.418556Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Load the dataset and setup values to start predictions\n","    Parameters: name of the file to retrieve\n","    Returns: the created dataframe, boundaries to start fetching for predictions\n","\"\"\"\n","def load_dataset(file_name):\n","    # Retrieve pathes\n","    file_path = os.path.join(df_input_dir, file_name)\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name.split('.')[0]) + '.txt'\n","    \n","    # Load the JSON file into a pandas DataFrame\n","    df = pd.read_json(file_path)\n","\n","    # Report the number of sentences\n","    print('Number of sentences: {:,}\\n'.format(df.shape[0]))\n","    \n","    # Read file with intermediate predictions, count the number of rows and set min_retrieved_data\n","    max_retrieved_data = df.shape[0] \n","    min_retrieved_data = 0\n","\n","    if os.path.isfile(intermediate_predictions_path):\n","        with open(intermediate_predictions_path, 'r') as file:\n","            min_retrieved_data = sum(1 for _ in file)\n","    print(\"Resume prediction from row #\", min_retrieved_data)\n","    \n","    return df, max_retrieved_data, min_retrieved_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.435768Z","iopub.status.busy":"2023-07-11T22:29:47.434774Z","iopub.status.idle":"2023-07-11T22:29:47.458969Z","shell.execute_reply":"2023-07-11T22:29:47.457160Z","shell.execute_reply.started":"2023-07-11T22:29:47.435723Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Define the steps to tokenize the input and perform classification\n","    Parameters: name of the file to retrieve, dataframe on which operating, boundaries to start fetching for predictions\n","    Returns: Nothing -> Predictions are stored in the file intermediate_predictions_path\n","\"\"\"\n","def make_classification(file_name, df, max_retrieved_data, min_retrieved_data):\n","    if min_retrieved_data >= max_retrieved_data:\n","        print(\"Classification already completed\")\n","        return\n","    \n","    # Retrieve path\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name.split('.')[0]) + '.txt'\n","    \n","    input_ids = []\n","    attention_masks = []\n","\n","    # Calculate the number of batches based on the range\n","    num_batches = math.ceil((max_retrieved_data - min_retrieved_data) / batch_size)\n","\n","    # Evaluate time elapsed\n","    start = time.time()\n","    print(\"Start tokenization\")\n","\n","    # Populate the inputs_list and indexes_list\n","    #for batch_index in range(num_batches):\n","    for batch_index in tqdm(range(num_batches), desc=\"Tokenization\"):\n","        start_index = min_retrieved_data + (batch_index * batch_size)\n","        end_index = min(start_index + batch_size -1, max_retrieved_data-1)\n","\n","        batch_input_texts = df.loc[start_index:end_index, 'processedContent'].tolist()\n","\n","        # Encode the batch of input texts\n","        batch_inputs = tokenizer.batch_encode_plus(\n","            batch_input_texts,\n","            add_special_tokens=True,\n","            max_length=window_length,\n","            pad_to_max_length=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        # Append the encoded input ids and attention masks to the respective lists\n","        input_ids.append(batch_inputs['input_ids'])\n","        attention_masks.append(batch_inputs['attention_mask'])\n","\n","    # Convert the lists into tensors\n","    input_ids = torch.cat(input_ids, dim=0)\n","    attention_masks = torch.cat(attention_masks, dim=0)\n","\n","    # Create the DataLoader\n","    prediction_data = TensorDataset(input_ids, attention_masks)\n","    prediction_sampler = SequentialSampler(prediction_data)\n","    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","\n","    print(\"Elapsed time for tokenization \", time.time() - start)\n","    print(\"Start classification\")\n","\n","    # Iterate over the data_loader\n","    #for batch in prediction_dataloader:\n","    for batch in tqdm(prediction_dataloader, desc=\"Processing\"):\n","\n","        predictions = []\n","\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # Unpack the inputs from the dataloader\n","        b_input_ids, b_input_mask = batch\n","\n","        # Speeding up prediction\n","        with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          result = model(b_input_ids,\n","                         token_type_ids=None,\n","                         attention_mask=b_input_mask,\n","                         return_dict=True)\n","\n","        logits = result.logits\n","\n","        # Move logits to CPU\n","        logits = logits.detach().cpu().numpy()\n","\n","        # Store predictions\n","        predictions.append(logits)\n","\n","        # Save intermediate predictions in a txt file\n","        temp = []\n","        res = []\n","        temp = np.concatenate(predictions, axis=0)\n","\n","        for i in range(0, len(temp)):\n","            # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n","            # and one column for \"1\"). Pick the label with the highest value and turn this\n","            # in to a list of 0s and 1s\n","            pred_labels_i = np.argmax(temp[i], axis=0).flatten()\n","            res.append(pred_labels_i[0])\n","\n","        with open(intermediate_predictions_path, 'a') as file:\n","                np.savetxt(file, res, fmt='%d')\n","                #print(\"Batch saved\")\n","\n","    end = time.time()\n","    print(\"Elapsed time for classification: [s]\", round(end - start, 2))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.461884Z","iopub.status.busy":"2023-07-11T22:29:47.461244Z","iopub.status.idle":"2023-07-11T22:29:47.481339Z","shell.execute_reply":"2023-07-11T22:29:47.480061Z","shell.execute_reply.started":"2023-07-11T22:29:47.461821Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Show the statistics of the classified posts and save them in a json file then release the dataframe\n","    Parameters: name of the file to retrieve, dataframe on which operating, upper boundary\n","    Returns: Nothing -> Predictions are stored in the file intermediate_predictions_path\n","\"\"\"\n","def save_predictions(file_name, df, max_retrieved_data):\n","    # Retrieve pathes\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name.split('.')[0]) + '.txt'\n","    json_output_path = os.path.join(json_output_dir, file_name)\n","    \n","    # After all posts are predicted, retrieve all their labels\n","    res = np.loadtxt(intermediate_predictions_path, dtype=int)\n","\n","    # Analyze labels\n","    count_ones = np.count_nonzero(res == 1)\n","    count_zeros = np.count_nonzero(res == 0)\n","\n","    print(\"Number of not CTI-relevant posts: \", count_zeros)\n","    print(\"Number of CTI-relevant posts: \", count_ones)\n","\n","    # Update CTIrelevant field\n","    df.loc[:max_retrieved_data - 1, 'CTIrelevant'] = res\n","    \n","    # Save files\n","    print(\"Saving json to %s\\n\" % json_output_path)\n","\n","    # Save the DataFrame to a JSON file in the specified directory\n","    df.to_json(json_output_path, orient='records', indent=4)\n","    \n","    # Free resources\n","    del df"]},{"cell_type":"markdown","metadata":{},"source":["### Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.485863Z","iopub.status.busy":"2023-07-11T22:29:47.485453Z","iopub.status.idle":"2023-07-11T22:29:47.501809Z","shell.execute_reply":"2023-07-11T22:29:47.500162Z","shell.execute_reply.started":"2023-07-11T22:29:47.485831Z"},"trusted":true},"outputs":[],"source":["def execute_code(filename):\n","    print(\"Retrieve\", filename)\n","    \n","    # Check if an output_json already exists for the given file -> if yes, skip to the next one\n","    json_output_path = os.path.join(json_output_dir, filename)\n","    if os.path.isfile(json_output_path):\n","        print(\"File already processed, skipped\")\n","        return\n","        \n","    # Load the dataset from df_input_dir\n","    df_unlabeled, max_retrieved_data, min_retrieved_data = load_dataset(filename)\n","    \n","    # Classification\n","    make_classification(filename, df_unlabeled, max_retrieved_data, min_retrieved_data)\n","    \n","    # Save predictions\n","    save_predictions(filename, df_unlabeled, max_retrieved_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.504276Z","iopub.status.busy":"2023-07-11T22:29:47.503581Z","iopub.status.idle":"2023-07-11T22:29:47.515112Z","shell.execute_reply":"2023-07-11T22:29:47.514230Z","shell.execute_reply.started":"2023-07-11T22:29:47.504176Z"},"trusted":true},"outputs":[],"source":["choose_interval = True  # True if one wants to execute a specified interval of chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.516946Z","iopub.status.busy":"2023-07-11T22:29:47.516395Z","iopub.status.idle":"2023-07-11T22:29:47.529996Z","shell.execute_reply":"2023-07-11T22:29:47.528149Z","shell.execute_reply.started":"2023-07-11T22:29:47.516913Z"},"trusted":true},"outputs":[],"source":["if choose_interval:\n","    for i in range(16,23):\n","        idx = str(i)\n","        filename = 'chunk'+idx+'_other_entries_not_labeled_processed.json'\n","        execute_code(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-11T22:29:47.532448Z","iopub.status.busy":"2023-07-11T22:29:47.531984Z"},"trusted":true},"outputs":[],"source":["if not choose_interval:\n","    for filename in os.listdir(df_input_dir):\n","        execute_code(filename)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
