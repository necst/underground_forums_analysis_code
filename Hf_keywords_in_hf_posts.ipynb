{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a695e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:02.512203Z",
     "iopub.status.busy": "2023-10-31T15:24:02.511582Z",
     "iopub.status.idle": "2023-10-31T15:24:14.152508Z",
     "shell.execute_reply": "2023-10-31T15:24:14.151401Z"
    },
    "papermill": {
     "duration": 11.650585,
     "end_time": "2023-10-31T15:24:14.155077",
     "exception": false,
     "start_time": "2023-10-31T15:24:02.504492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installation for IOC defanging - https://github.com/ioc-fang/ioc-fanger\n",
    "!pip install -q ioc-fanger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6146f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:14.167524Z",
     "iopub.status.busy": "2023-10-31T15:24:14.167132Z",
     "iopub.status.idle": "2023-10-31T15:24:16.288190Z",
     "shell.execute_reply": "2023-10-31T15:24:16.286746Z"
    },
    "papermill": {
     "duration": 2.130333,
     "end_time": "2023-10-31T15:24:16.290702",
     "exception": false,
     "start_time": "2023-10-31T15:24:14.160369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "import ioc_fanger\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cdb385",
   "metadata": {
    "papermill": {
     "duration": 0.004906,
     "end_time": "2023-10-31T15:24:16.300915",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.296009",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Generate complete list of HF keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba406e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:16.313623Z",
     "iopub.status.busy": "2023-10-31T15:24:16.313026Z",
     "iopub.status.idle": "2023-10-31T15:24:16.808509Z",
     "shell.execute_reply": "2023-10-31T15:24:16.807007Z"
    },
    "papermill": {
     "duration": 0.504343,
     "end_time": "2023-10-31T15:24:16.810693",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.306350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tags = [\"MAL\", \"FILE\", \"OS\", \"PROT\", \"MD5\", \"SHA1\", \"SHA2\",  \"ENCR\", \"TOOL\", \"VULID\", \"VULNAME\",\n",
    "        \"ACT\", \"APT\", \"SECTEAM\", \"IDTY\", \"EMAIL\", \"IP\", \"DOM\", \"URL\",  ]  #\"LOC\", \"TIME\", \n",
    "\n",
    "chunk_size = 500\n",
    "\n",
    "def save_chunk(output_directory, count, chunk):\n",
    "    output_file = os.path.join(output_directory, f\"chunk_{count}.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(chunk))\n",
    "\n",
    "output_directory = 'new_lists_folder'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "output_directory = 'chunks_complete_entity_list'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "complete_list = set()\n",
    "idx_file = 1\n",
    "count = 0\n",
    "chunk = []\n",
    "tag_list = []\n",
    "for t in tags:\n",
    "    tag = t\n",
    "    print(f'Tag {t} first file index {idx_file}')\n",
    "    countl=0\n",
    "    new_list = []\n",
    "    file_path = f'/home/anon/input/datasets-extracted-entities/HF_extracted_entities/entity_lists/{t}_list.txt'\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        new_file = []\n",
    "        for line in file:\n",
    "            new_file.append(line.strip())\n",
    "        new_file = sorted(new_file, key=lambda x: (-len(x), x))\n",
    "        for line in new_file:\n",
    "            countl += 1\n",
    "            line = line.strip()\n",
    "            if line not in complete_list:\n",
    "                complete_list.add(line)\n",
    "                new_list.append(line)\n",
    "                \n",
    "                chunk.append(line)\n",
    "                count += 1\n",
    "                if count % chunk_size == 0:\n",
    "                    save_chunk('chunks_complete_entity_list', idx_file, chunk)\n",
    "                    tag_list.append(tag)\n",
    "                    chunk = []\n",
    "                    idx_file += 1                    \n",
    "        if chunk:\n",
    "            save_chunk('chunks_complete_entity_list', idx_file, chunk)\n",
    "            tag_list.append(tag)\n",
    "            chunk = []\n",
    "            idx_file += 1 \n",
    "            count=0\n",
    "    \n",
    "    print(f'{t} - Before: {countl} - After: {len(new_list)}\\n')\n",
    "    \n",
    "    output_file = os.path.join('new_lists_folder', f\"{t}_list.txt\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        file.write('\\n'.join(new_list))\n",
    "\n",
    "print(f'Chunks: {idx_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7ea80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:16.823197Z",
     "iopub.status.busy": "2023-10-31T15:24:16.822850Z",
     "iopub.status.idle": "2023-10-31T15:24:16.831360Z",
     "shell.execute_reply": "2023-10-31T15:24:16.829856Z"
    },
    "papermill": {
     "duration": 0.017759,
     "end_time": "2023-10-31T15:24:16.833908",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.816149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(tag_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e07f4d",
   "metadata": {
    "papermill": {
     "duration": 0.005273,
     "end_time": "2023-10-31T15:24:16.844820",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.839547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Search HF keywords in HF posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ac80ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:16.857921Z",
     "iopub.status.busy": "2023-10-31T15:24:16.857532Z",
     "iopub.status.idle": "2023-10-31T15:24:16.862776Z",
     "shell.execute_reply": "2023-10-31T15:24:16.861843Z"
    },
    "papermill": {
     "duration": 0.013639,
     "end_time": "2023-10-31T15:24:16.864549",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.850910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_directory_keywords = '/home/anon/working/chunks_complete_entity_list'\n",
    "input_path_HF = '/home/anon/input/datasets-extracted-entities/HF_extracted_entities/HF_extracted_entities.json'\n",
    "output_directory = '/home/anon/working/chunks_HF_keywords_in_HF_posts/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a859e",
   "metadata": {
    "papermill": {
     "duration": 0.006426,
     "end_time": "2023-10-31T15:24:16.876464",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.870038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load HF dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77f998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:16.888841Z",
     "iopub.status.busy": "2023-10-31T15:24:16.888451Z",
     "iopub.status.idle": "2023-10-31T15:24:29.260375Z",
     "shell.execute_reply": "2023-10-31T15:24:29.258973Z"
    },
    "papermill": {
     "duration": 12.380633,
     "end_time": "2023-10-31T15:24:29.262475",
     "exception": false,
     "start_time": "2023-10-31T15:24:16.881842",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load HF posts\n",
    "df_HF = pd.read_json(input_path_HF)\n",
    "\n",
    "# Convert title and content columns to lowercase for efficient matching\n",
    "df_HF['threadTitle'] = df_HF['threadTitle'].str.lower()\n",
    "df_HF['flatContent'] = df_HF['flatContent'].str.lower()\n",
    "print('Number of relevant posts: ', df_HF.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d3d4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:29.275644Z",
     "iopub.status.busy": "2023-10-31T15:24:29.275272Z",
     "iopub.status.idle": "2023-10-31T15:24:29.281897Z",
     "shell.execute_reply": "2023-10-31T15:24:29.280455Z"
    },
    "papermill": {
     "duration": 0.015793,
     "end_time": "2023-10-31T15:24:29.284054",
     "exception": false,
     "start_time": "2023-10-31T15:24:29.268261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def defang_iocs_in_text(text):\n",
    "    defanged_text = ioc_fanger.defang(text)\n",
    "    return defanged_text\n",
    "\n",
    "def fang_iocs_in_text(text):\n",
    "    fanged_text = ioc_fanger.fang(text)\n",
    "    return fanged_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42538928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:29.297334Z",
     "iopub.status.busy": "2023-10-31T15:24:29.296999Z",
     "iopub.status.idle": "2023-10-31T15:24:57.814039Z",
     "shell.execute_reply": "2023-10-31T15:24:57.812563Z"
    },
    "papermill": {
     "duration": 28.526449,
     "end_time": "2023-10-31T15:24:57.816363",
     "exception": false,
     "start_time": "2023-10-31T15:24:29.289914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url_pattern = r'http\\S+|www\\S+|https\\S+|h\\*\\*p\\S+|h\\*\\*ps\\S+'  # Starting with http , https , www, h**p, h**ps\n",
    "delimiter_pattern = r'\\s+|[:,;?!{}\\[\\]=|]+'  # To split\n",
    "\n",
    "def process_content(row):  # Try to preserve URLs while tokenizing - Reproducing splitting used to process content to extract kw HF side\n",
    "    #content = row['threadTitle'] + ' ' + row['flatContent']\n",
    "    content = ' '.join(row['content']).lower()\n",
    "    tokens = re.split(f\"({url_pattern})\", content)  # Split the text by URLs and non-URLs\n",
    "    tokens = [token.strip() for token in tokens if token.strip()]   # Remove empty tokens and trim whitespace\n",
    "\n",
    "    # Split non-link tokens into sub_tokens\n",
    "    split_tokens = []\n",
    "    for token in tokens:\n",
    "        if not re.match(url_pattern, token):\n",
    "            sub_tokens = re.split(delimiter_pattern, token)\n",
    "            split_tokens.extend(sub_tokens)\n",
    "        else:\n",
    "            split_tokens.append(token)\n",
    "\n",
    "    return ' '.join(set(split_tokens))\n",
    "\n",
    "# Function to merge entity lists\n",
    "def merge_entity_lists(entities):\n",
    "    merged_list = []\n",
    "    for entity_type, entity_list in entities.items():\n",
    "        merged_list.extend([fang_iocs_in_text(el).lower() for el in entity_list])\n",
    "    return merged_list\n",
    "\n",
    "# Initialize tqdm progress bar\n",
    "pbar = tqdm(total=len(df_HF))\n",
    "\n",
    "def apply_process_content(row):\n",
    "    global pbar\n",
    "    pbar.update(1)  # Update tqdm progress bar\n",
    "    return process_content(row)\n",
    "\n",
    "df_HF['tokenizedContent'] = df_HF.apply(apply_process_content, axis=1)\n",
    "\n",
    "# Close tqdm progress bar\n",
    "pbar.close()\n",
    "\n",
    "df_HF['date'] = pd.to_datetime(df_HF['date'])\n",
    "df_HF['date'] = df_HF['date'].dt.strftime('%m-%d-%Y')\n",
    "\n",
    "# Sort the DataFrame by the 'date' column\n",
    "df_HF.sort_values(by='date', inplace=True)\n",
    "\n",
    "# Apply the merge_entity_lists function to each row and create a new column\n",
    "df_HF['merged_entities'] = df_HF['entities'].apply(merge_entity_lists)\n",
    "\n",
    "# Filter the DataFrame to only keep the selected columns\n",
    "df_HF = df_HF[['ID', 'date', 'merged_entities', 'tokenizedContent']]\n",
    "\n",
    "# Convert the filtered DataFrame to a list of dictionaries\n",
    "existing_json_data = df_HF.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e559dd",
   "metadata": {
    "papermill": {
     "duration": 0.009818,
     "end_time": "2023-10-31T15:24:57.836336",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.826518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Search with entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b15df2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:57.859159Z",
     "iopub.status.busy": "2023-10-31T15:24:57.858780Z",
     "iopub.status.idle": "2023-10-31T15:24:57.864801Z",
     "shell.execute_reply": "2023-10-31T15:24:57.863486Z"
    },
    "papermill": {
     "duration": 0.020559,
     "end_time": "2023-10-31T15:24:57.866965",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.846406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_keyword(keyword):\n",
    "    keyword_list = []\n",
    "    for entry in existing_json_data:\n",
    "        if keyword in entry['merged_entities']:\n",
    "            keyword_list.append({\n",
    "                'ID': entry['ID'],\n",
    "                'date': entry['date']\n",
    "            })\n",
    "    return keyword, keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33bf2fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:57.888737Z",
     "iopub.status.busy": "2023-10-31T15:24:57.888355Z",
     "iopub.status.idle": "2023-10-31T15:24:57.897415Z",
     "shell.execute_reply": "2023-10-31T15:24:57.896094Z"
    },
    "papermill": {
     "duration": 0.022875,
     "end_time": "2023-10-31T15:24:57.899842",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.876967",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define regular expression patterns\n",
    "ip_pattern = re.compile(r'^(\\d+\\.\\d+\\.\\d+\\.\\d+)$')\n",
    "pattern_to_remove = re.compile(r'^[-\\'\\+,/\\\\:=_\\|\\.\\^~0-9]+$')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62490b15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:57.922489Z",
     "iopub.status.busy": "2023-10-31T15:24:57.921887Z",
     "iopub.status.idle": "2023-10-31T15:24:57.928099Z",
     "shell.execute_reply": "2023-10-31T15:24:57.927054Z"
    },
    "papermill": {
     "duration": 0.019917,
     "end_time": "2023-10-31T15:24:57.930340",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.910423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_HF_keywords_in_HF(keywords_path):\n",
    "    print(f'Working on {keywords_path}')\n",
    "    # Open the set of keywords\n",
    "    with open(keywords_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    # Remove newlines and create a list\n",
    "    keywords_list = [line.strip() for line in lines]\n",
    "    # Filter and keep only the words that match the patterns\n",
    "    keywords_list = [keyword for keyword in keywords_list\n",
    "                     if ip_pattern.match(keyword) or not (pattern_to_remove.match(keyword) or keyword in stop_words or len(keyword) == 1)]\n",
    "\n",
    "    # Create a dictionary to hold the reports by keyword\n",
    "    return {keyword: [] for keyword in keywords_list}, keywords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534385c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:57.953283Z",
     "iopub.status.busy": "2023-10-31T15:24:57.952613Z",
     "iopub.status.idle": "2023-10-31T15:24:57.960103Z",
     "shell.execute_reply": "2023-10-31T15:24:57.959317Z"
    },
    "papermill": {
     "duration": 0.02178,
     "end_time": "2023-10-31T15:24:57.962189",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.940409",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def start_searching(HF_keywords_in_HF_dict, tag, keywords_list):\n",
    "    # Use ThreadPoolExecutor to parallelize keyword processing\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit tasks for each keyword\n",
    "        keyword_futures = {executor.submit(process_keyword, keyword): keyword for keyword in keywords_list}\n",
    "\n",
    "        # Use tqdm to track progress\n",
    "        with tqdm(total=len(keywords_list), desc='Searching in entities') as pbar:\n",
    "            for future in concurrent.futures.as_completed(keyword_futures):\n",
    "                keyword = keyword_futures[future]\n",
    "                keyword_results = future.result()\n",
    "                HF_keywords_in_HF_dict[keyword] = keyword_results\n",
    "                pbar.update(1)  # Update progress bar\n",
    "    \n",
    "    # Save resulting list\n",
    "    final_structure = []\n",
    "\n",
    "    for keyword, value_list in HF_keywords_in_HF_dict.items():\n",
    "            final_structure.append({\n",
    "                'keyword': keyword,\n",
    "                'tag': tag,\n",
    "                'matching_posts': value_list[1]\n",
    "            })\n",
    "            \n",
    "    return final_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe68469",
   "metadata": {
    "papermill": {
     "duration": 0.010133,
     "end_time": "2023-10-31T15:24:57.982478",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.972345",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Search with posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2aa35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:58.005050Z",
     "iopub.status.busy": "2023-10-31T15:24:58.004433Z",
     "iopub.status.idle": "2023-10-31T15:24:58.016465Z",
     "shell.execute_reply": "2023-10-31T15:24:58.015460Z"
    },
    "papermill": {
     "duration": 0.025664,
     "end_time": "2023-10-31T15:24:58.018542",
     "exception": false,
     "start_time": "2023-10-31T15:24:57.992878",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_lines_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield line.strip()\n",
    "\n",
    "def create_set_from_file(file_path):\n",
    "    return set(read_lines_from_file(file_path))\n",
    "\n",
    "def create_bulk_search_pattern(keywords_list):\n",
    "    keywords_list = sorted(keywords_list, key=lambda x: (-len(x), x))\n",
    "    #keyword_regex = r\"\\b(?:\" + \"|\".join(re.escape(word) for word in keywords_list) + r\")\\b\"\n",
    "    keyword_regex = r\"(^|\\s)[^a-zA-Z0-9]*(\" + \"|\".join(re.escape(word.lower()) for word in keywords_list) + r\")[^a-zA-Z0-9]*(\\s|$)\"\n",
    "    return re.compile(keyword_regex)\n",
    "\n",
    "def search_matches_in_chunk(chunk, bulk_search_pattern):\n",
    "    word_matching = defaultdict(list)\n",
    "    for post in chunk.itertuples(index=False):\n",
    "        content = post.tokenizedContent.lower()\n",
    "\n",
    "        matches = bulk_search_pattern.findall(content)\n",
    "        for word in matches:\n",
    "            matched_word = word[1].lower()\n",
    "            word_matching[matched_word].append({\n",
    "                'ID': post.ID,\n",
    "                'date': post.date\n",
    "            })\n",
    "    return word_matching\n",
    "\n",
    "def search_matches(input_file_path, output_file_path, df_HF):\n",
    "    # Load keywords of reports\n",
    "    keywords_list = create_set_from_file(input_file_path)\n",
    "\n",
    "    # Convert keywords_list to lowercase for efficient matching\n",
    "    keywords_list_lower = {word.lower() for word in keywords_list}\n",
    "\n",
    "    # Compile the bulk search pattern\n",
    "    bulk_search_pattern = create_bulk_search_pattern(keywords_list_lower)\n",
    "\n",
    "    # Divide the dataframe into smaller chunks for parallel processing\n",
    "    chunk_size = 1000\n",
    "    df_chunks = [df_HF[i:i+chunk_size] for i in range(0, df_HF.shape[0], chunk_size)]\n",
    "                \n",
    "    # Use ThreadPoolExecutor for parallel processing\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(search_matches_in_chunk, chunk, bulk_search_pattern) for chunk in df_chunks]\n",
    "        word_matching_dict = {}\n",
    "        for future in tqdm(futures, total=len(futures), desc='Searching in posts'):\n",
    "            chunk_word_matching = future.result()\n",
    "            for word, matches in chunk_word_matching.items():\n",
    "                if word in word_matching_dict:\n",
    "                    word_matching_dict[word].extend(matches)\n",
    "                else:\n",
    "                    word_matching_dict[word] = matches\n",
    "\n",
    "    # Convert word_matching_dict to a list of dictionaries\n",
    "    word_matching = [{'keyword': word, 'matching posts': matches} for word, matches in word_matching_dict.items()]\n",
    "\n",
    "    return word_matching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f82c8b",
   "metadata": {
    "papermill": {
     "duration": 0.01003,
     "end_time": "2023-10-31T15:24:58.038666",
     "exception": false,
     "start_time": "2023-10-31T15:24:58.028636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00a5c22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:58.061430Z",
     "iopub.status.busy": "2023-10-31T15:24:58.060862Z",
     "iopub.status.idle": "2023-10-31T15:24:58.069260Z",
     "shell.execute_reply": "2023-10-31T15:24:58.068053Z"
    },
    "papermill": {
     "duration": 0.022369,
     "end_time": "2023-10-31T15:24:58.071368",
     "exception": false,
     "start_time": "2023-10-31T15:24:58.048999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_struct_and_save(struct_from_entity, struct_from_posts, output_path):\n",
    "    for entry in struct_from_posts:\n",
    "        kw = entry['keyword']\n",
    "        mp = entry['matching posts']\n",
    "        \n",
    "        for idx, item in enumerate(struct_from_entity):\n",
    "            if item['keyword'] == kw:\n",
    "                item['matching_posts'].extend(mp)\n",
    "                break\n",
    "                \n",
    "    \n",
    "    # Create a dictionary to store unique IDs for each keyword using sets\n",
    "    unique_ids = {}\n",
    "    # Iterate through the data and remove duplicates using sets\n",
    "    for item in tqdm(struct_from_entity, 'Removing duplicates'):\n",
    "        keyword = item[\"keyword\"]\n",
    "        #tag = item[\"tag\"]\n",
    "        matching_posts = item[\"matching_posts\"]\n",
    "        # Create a set to store unique IDs for the current keyword\n",
    "        unique_ids[keyword] = set()\n",
    "        # Create a list to store unique posts for the current keyword\n",
    "        unique_posts = []\n",
    "        for post in matching_posts:\n",
    "            post_id = post[\"ID\"]\n",
    "            # Check if the ID is not in the unique_ids set for this keyword\n",
    "            if post_id not in unique_ids[keyword]:\n",
    "                # Add the ID to the unique_ids set\n",
    "                unique_ids[keyword].add(post_id)\n",
    "                # Add the post to the unique_posts list\n",
    "                unique_posts.append(post)\n",
    "        # Update the matching posts for the current keyword\n",
    "        item[\"matching_posts\"] = unique_posts\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(struct_from_entity, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a4c69",
   "metadata": {
    "papermill": {
     "duration": 0.009714,
     "end_time": "2023-10-31T15:24:58.091084",
     "exception": false,
     "start_time": "2023-10-31T15:24:58.081370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4668d87d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:24:58.113480Z",
     "iopub.status.busy": "2023-10-31T15:24:58.113135Z",
     "iopub.status.idle": "2023-10-31T15:43:30.159606Z",
     "shell.execute_reply": "2023-10-31T15:43:30.158493Z"
    },
    "papermill": {
     "duration": 1112.061028,
     "end_time": "2023-10-31T15:43:30.161939",
     "exception": false,
     "start_time": "2023-10-31T15:24:58.100911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = 1\n",
    "end = start + 29  \n",
    "#(1, 30) - (30, 59) - (59, 88) - (88, 117) - (117, 146) - (146, 175) - (175, 204) - (204, 233) - (233, 262) - (262, 281)\n",
    "\n",
    "end = min(end, 281)\n",
    "\n",
    "print('Searching from {0} to {1}'.format(start, end-1))\n",
    "\n",
    "for idx_file in range(start, end):  \n",
    "    iFilename = 'chunk_{0}.txt'.format(idx_file)\n",
    "    input_path = os.path.join(input_directory_keywords, iFilename)    \n",
    "    HF_keywords_in_HF_dict, keywords_list = get_HF_keywords_in_HF(input_path)\n",
    "\n",
    "    oFilename = 'HF_keywords_in_HF_posts_{0}.json'.format(idx_file)\n",
    "    output_path = os.path.join(output_directory, oFilename)\n",
    "    tag = tag_list[idx_file-1]\n",
    "\n",
    "    struct_from_entity = start_searching(HF_keywords_in_HF_dict, tag, keywords_list)\n",
    "    struct_from_posts = search_matches(input_path, output_path, df_HF)\n",
    "    \n",
    "    compare_struct_and_save(struct_from_entity, struct_from_posts, output_path)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0642dd",
   "metadata": {
    "papermill": {
     "duration": 0.277975,
     "end_time": "2023-10-31T15:43:30.714004",
     "exception": false,
     "start_time": "2023-10-31T15:43:30.436029",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Save as ZIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2da40e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-31T15:43:31.266347Z",
     "iopub.status.busy": "2023-10-31T15:43:31.264661Z",
     "iopub.status.idle": "2023-10-31T15:43:31.836178Z",
     "shell.execute_reply": "2023-10-31T15:43:31.834787Z"
    },
    "papermill": {
     "duration": 0.849673,
     "end_time": "2023-10-31T15:43:31.838925",
     "exception": false,
     "start_time": "2023-10-31T15:43:30.989252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directory path containing the output files to zip\n",
    "source_directory = output_directory\n",
    "\n",
    "# Zip file path\n",
    "zip_file_path = '/home/anon/working/HF_keywords_in_HF_posts.zip'\n",
    "\n",
    "# Create a ZIP archive\n",
    "with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    # Loop through all files in the directory and add them to the ZIP archive\n",
    "    for root, _, files in os.walk(source_directory):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(root, file)\n",
    "            zipf.write(file_path, os.path.relpath(file_path, source_directory))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1181.159351,
   "end_time": "2023-10-31T15:43:33.561602",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-31T15:23:52.402251",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
