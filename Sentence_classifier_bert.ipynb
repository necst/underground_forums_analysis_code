{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:16.361112Z","iopub.status.busy":"2023-10-09T11:19:16.360715Z","iopub.status.idle":"2023-10-09T11:19:31.318574Z","shell.execute_reply":"2023-10-09T11:19:31.317647Z","shell.execute_reply.started":"2023-10-09T11:19:16.361088Z"},"trusted":true},"outputs":[],"source":["import os\n","import json\n","import time\n","import torch\n","import random\n","import zipfile\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from datasets import load_dataset\n","from transformers import BertTokenizer\n","from torch.utils.data import TensorDataset\n","from sklearn.metrics import roc_curve, auc\n","from sklearn.metrics import matthews_corrcoef\n","from sklearn.model_selection import train_test_split\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:31.320848Z","iopub.status.busy":"2023-10-09T11:19:31.320335Z","iopub.status.idle":"2023-10-09T11:19:31.466514Z","shell.execute_reply":"2023-10-09T11:19:31.465635Z","shell.execute_reply.started":"2023-10-09T11:19:31.320817Z"},"trusted":true},"outputs":[],"source":["sentences_list = []\n","\n","# Specify the file path\n","file_path = '/home/anon/input/relevant-sentences-dataset/datasetBert - Productive.csv'\n","# Open the file in read mode\n","with open(file_path, 'r') as file:\n","    for line in file:\n","        line = line.strip()\n","        val = 1\n","        sent = line[2:]\n","        if sent[0] == '\"':\n","            sent = sent [1:-1]\n","        sentences_list.append({'sentence': sent, 'productive': val})\n","\n","# Specify the file path\n","file_path = '/home/anon/input/relevant-sentences-dataset/datasetBert - Non Productive.csv'\n","# Open the file in read mode\n","with open(file_path, 'r') as file:\n","    for line in file:\n","        line = line.strip()\n","        val = 1\n","        sent = line[2:]\n","        if sent[0] == '\"':\n","            sent = sent [1:-1]\n","        sentences_list.append({'sentence': sent, 'productive': val})\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:31.468216Z","iopub.status.busy":"2023-10-09T11:19:31.467880Z","iopub.status.idle":"2023-10-09T11:19:38.887637Z","shell.execute_reply":"2023-10-09T11:19:38.886645Z","shell.execute_reply.started":"2023-10-09T11:19:31.468169Z"},"trusted":true},"outputs":[],"source":["dataset = load_dataset(\"conll2003\")\n","\n","for el in dataset['train']:\n","    sent = ' '.join(el['tokens'])\n","    val = 0\n","    sentences_list.append({'sentence': sent, 'productive': val})\n","\n","for el in dataset['validation']:\n","    sent = ' '.join(el['tokens'])\n","    val = 0\n","    sentences_list.append({'sentence': sent, 'productive': val})\n","\n","for el in dataset['test']:\n","    sent = ' '.join(el['tokens'])\n","    val = 0\n","    sentences_list.append({'sentence': sent, 'productive': val})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:38.890459Z","iopub.status.busy":"2023-10-09T11:19:38.890096Z","iopub.status.idle":"2023-10-09T11:19:39.029092Z","shell.execute_reply":"2023-10-09T11:19:39.028171Z","shell.execute_reply.started":"2023-10-09T11:19:38.890429Z"},"trusted":true},"outputs":[],"source":["with open('Productive_sentences.json', 'w') as json_file:\n","    json.dump(sentences_list, json_file, indent=4)"]},{"cell_type":"markdown","metadata":{"id":"bXvcF2bbyqCL"},"source":["### Setup for GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:39.031332Z","iopub.status.busy":"2023-10-09T11:19:39.030697Z","iopub.status.idle":"2023-10-09T11:19:39.038526Z","shell.execute_reply":"2023-10-09T11:19:39.037550Z","shell.execute_reply.started":"2023-10-09T11:19:39.031298Z"},"id":"VRsnTpAQwmlA","outputId":"43f07993-987d-48a1-82c7-3d85d7fa2348","trusted":true},"outputs":[],"source":["\"\"\"# To confirm that the GPU is detected\n","\n","import tensorflow as tf\n","\n","# Get the GPU device name\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:39.040913Z","iopub.status.busy":"2023-10-09T11:19:39.040148Z","iopub.status.idle":"2023-10-09T11:19:39.079376Z","shell.execute_reply":"2023-10-09T11:19:39.078272Z","shell.execute_reply.started":"2023-10-09T11:19:39.040882Z"},"id":"pLsFUzYPw1ak","outputId":"212f8783-cf79-413c-dabe-9006cf3b345b","trusted":true},"outputs":[],"source":["# To identify and specify the GPU\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():\n","\n","    # Tell PyTorch to use the GPU\n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"h120nrZtyxJm"},"source":["### Installing the Hugging Face Library"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:39.081518Z","iopub.status.busy":"2023-10-09T11:19:39.080886Z","iopub.status.idle":"2023-10-09T11:19:49.136681Z","shell.execute_reply":"2023-10-09T11:19:49.135601Z","shell.execute_reply.started":"2023-10-09T11:19:39.081483Z"},"id":"UmqGaeD0xl1K","outputId":"a712e237-b63d-4386-9dc7-61d149e4966d","trusted":true},"outputs":[],"source":["!pip install -q transformers"]},{"cell_type":"markdown","metadata":{},"source":["### Setup parameters\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:49.138740Z","iopub.status.busy":"2023-10-09T11:19:49.138425Z","iopub.status.idle":"2023-10-09T11:19:49.143867Z","shell.execute_reply":"2023-10-09T11:19:49.143012Z","shell.execute_reply.started":"2023-10-09T11:19:49.138714Z"},"trusted":true},"outputs":[],"source":["# Seed for reproducibility\n","seed_val = 42\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4\n","epochs = 4\n","\n","# Define the maximum sequence length for each window\n","window_length = 128\n","\n","# The DataLoader needs to know the batch size for training\n","# For fine-tuning BERT on a specific task, the authors recommend a batch\n","# size of 16 or 32\n","batch_size = 32"]},{"cell_type":"markdown","metadata":{"id":"CI7oaf4YzFqP"},"source":["### Load the labeld dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:49.145444Z","iopub.status.busy":"2023-10-09T11:19:49.145098Z","iopub.status.idle":"2023-10-09T11:19:49.255151Z","shell.execute_reply":"2023-10-09T11:19:49.254066Z","shell.execute_reply.started":"2023-10-09T11:19:49.145416Z"},"id":"oMnGf2UnxvRP","outputId":"d18afe1d-987e-4132-f594-46832edfd61e","trusted":true},"outputs":[],"source":["# Load the JSON file into a pandas DataFrame\n","df = pd.read_json('/home/anon/working/Productive_sentences.json')\n","\n","df['sentence'] = df['sentence'].str.lower()\n","df['productive'] = df['productive'].astype(int)\n","\n","# Report the number of sentences\n","print('Number of sentences: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 5 random rows from the data\n","df.sample(5)"]},{"cell_type":"markdown","metadata":{"id":"QYCUkIdiYMXd"},"source":["### Split train - validation - test sets"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:49.259937Z","iopub.status.busy":"2023-10-09T11:19:49.259686Z","iopub.status.idle":"2023-10-09T11:19:49.286547Z","shell.execute_reply":"2023-10-09T11:19:49.285585Z","shell.execute_reply.started":"2023-10-09T11:19:49.259916Z"},"id":"0oElO2o6VaR0","outputId":"7561ab75-9080-4480-ded0-d6b52b160387","trusted":true},"outputs":[],"source":["# Split the features and labels using stratified sampling\n","features_train, features_val_test, labels_train, labels_val_test = train_test_split(\n","    df['sentence'],      # Features\n","    df['productive'],           # Labels\n","    test_size=0.3,               # 30% for validation and test combined\n","    stratify=df['productive'],  # Stratified sampling based on labels\n","    random_state=seed_val        # Set a random seed for reproducibility\n",")\n","\n","# Split the validation and test sets\n","features_val, features_test, labels_val, labels_test = train_test_split(\n","    features_val_test, labels_val_test,\n","    test_size=0.333,             # Split remaining 30% into 20% for validation and 10% for test\n","    stratify=labels_val_test,    # Stratified sampling based on labels\n","    random_state=seed_val        # Set a random seed for reproducibility\n",")\n","\n","# Create dataframes for each split\n","df_train = df.loc[features_train.index]\n","df_val = df.loc[features_val.index]\n","df_test = df.loc[features_test.index]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:49.288511Z","iopub.status.busy":"2023-10-09T11:19:49.287983Z","iopub.status.idle":"2023-10-09T11:19:49.298965Z","shell.execute_reply":"2023-10-09T11:19:49.298033Z","shell.execute_reply.started":"2023-10-09T11:19:49.288482Z"},"trusted":true},"outputs":[],"source":["# Reset indices for features and labels\n","df_train = df_train.reset_index()\n","df_val = df_val.reset_index()\n","df_test = df_test.reset_index()\n","\n","features_train = features_train.reset_index(drop=True)\n","features_val = features_val.reset_index(drop=True)\n","features_test = features_test.reset_index(drop=True)\n","labels_train = labels_train.reset_index(drop=True)\n","labels_val = labels_val.reset_index(drop=True)\n","labels_test = labels_test.reset_index(drop=True)\n","\n","# Print the number of samples in each set\n","print('Training samples:', len(features_train))\n","print('Validation samples:', len(features_val))\n","print('Test samples:', len(features_test))"]},{"cell_type":"markdown","metadata":{"id":"Qj5_yK8w-dqk"},"source":["### BERT Tokenizer"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:49.300898Z","iopub.status.busy":"2023-10-09T11:19:49.300331Z","iopub.status.idle":"2023-10-09T11:19:50.210174Z","shell.execute_reply":"2023-10-09T11:19:50.209401Z","shell.execute_reply.started":"2023-10-09T11:19:49.300867Z"},"id":"eXy4FB-E_By_","outputId":"f5b5fb56-38bb-4b72-85d8-b042de9441aa","trusted":true},"outputs":[],"source":["# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"markdown","metadata":{},"source":["### Inspect possible number of tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:19:50.212172Z","iopub.status.busy":"2023-10-09T11:19:50.211340Z","iopub.status.idle":"2023-10-09T11:20:00.446640Z","shell.execute_reply":"2023-10-09T11:20:00.445775Z","shell.execute_reply.started":"2023-10-09T11:19:50.212140Z"},"trusted":true},"outputs":[],"source":["# Get length of all the messages in the dataset\n","seq_len = [len(tokenizer.tokenize(i)) for i in df['sentence']]\n","\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","pd.Series(seq_len).hist(bins = 30)\n","\n","# Display the histogram\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:00.448495Z","iopub.status.busy":"2023-10-09T11:20:00.447934Z","iopub.status.idle":"2023-10-09T11:20:00.704839Z","shell.execute_reply":"2023-10-09T11:20:00.703969Z","shell.execute_reply.started":"2023-10-09T11:20:00.448463Z"},"trusted":true},"outputs":[],"source":["# More specifically...\n","filtered_seq_len = [length for length in seq_len if length < 128]\n","\n","# Plot the histogram for filtered_seq_len\n","pd.Series(filtered_seq_len).hist(bins=30)\n","\n","# Display the histogram\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:00.706810Z","iopub.status.busy":"2023-10-09T11:20:00.706281Z","iopub.status.idle":"2023-10-09T11:20:00.714061Z","shell.execute_reply":"2023-10-09T11:20:00.713039Z","shell.execute_reply.started":"2023-10-09T11:20:00.706777Z"},"id":"_TEZ2zVB_PX8","outputId":"870550dd-672d-4052-a5ec-237121b7ca4b","trusted":true},"outputs":[],"source":["# Example of sentence\n","sentence = features_train[0]\n","\n","# Print the original sentence\n","print(' Original: ', sentence)\n","\n","# Print the sentence split into tokens\n","print('Tokenized: ', tokenizer.tokenize(sentence))\n","\n","# Print the sentence mapped to token ids\n","print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence)))"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenize train set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:00.716297Z","iopub.status.busy":"2023-10-09T11:20:00.715289Z","iopub.status.idle":"2023-10-09T11:20:12.249656Z","shell.execute_reply":"2023-10-09T11:20:12.248843Z","shell.execute_reply.started":"2023-10-09T11:20:00.716188Z"},"id":"xPUsWv7qEJXM","outputId":"905891c0-b6bb-4ba5-86d4-91a2bd72c223","trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sample in features_train:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence\n","    #   (2) Prepend the `[CLS]` token to the start\n","    #   (3) Append the `[SEP]` token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        sample,                       # Sentence to encode\n","                        add_special_tokens = True,    # Add '[CLS]' and '[SEP]'\n","                        max_length = window_length,   # Pad & truncate all sentences\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True, # Construct attn. masks\n","                        return_tensors = 'pt',        # Return pytorch tensors\n","                   )\n","\n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_train)\n","\n","# Print sentence 0, now as a list of IDs\n","print('Original: ', features_train[0])\n","print('Tokenized: ', tokenizer.tokenize(features_train[0]))\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:12.255441Z","iopub.status.busy":"2023-10-09T11:20:12.253437Z","iopub.status.idle":"2023-10-09T11:20:12.264323Z","shell.execute_reply":"2023-10-09T11:20:12.263529Z","shell.execute_reply.started":"2023-10-09T11:20:12.255408Z"},"trusted":true},"outputs":[],"source":["# Combine the training inputs into a TensorDataset\n","train_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Print the number of samples in the set\n","print('{:>5,} training samples'.format(len(train_dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:12.270105Z","iopub.status.busy":"2023-10-09T11:20:12.268614Z","iopub.status.idle":"2023-10-09T11:20:12.277976Z","shell.execute_reply":"2023-10-09T11:20:12.277106Z","shell.execute_reply.started":"2023-10-09T11:20:12.270073Z"},"trusted":true},"outputs":[],"source":["# Create an iterator for the dataset using the torch DataLoader class\n","\n","# Create the DataLoaders for the training set\n","# Take training samples in random order\n","train_dataloader = DataLoader(\n","            train_dataset,                          # The training samples\n","            sampler = RandomSampler(train_dataset), # Select batches randomly\n","            batch_size = batch_size                 # Trains with this batch size\n","        )"]},{"cell_type":"markdown","metadata":{"id":"t49i5pw5FCWf"},"source":["### Tokenize validation set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:12.282575Z","iopub.status.busy":"2023-10-09T11:20:12.281908Z","iopub.status.idle":"2023-10-09T11:20:15.689584Z","shell.execute_reply":"2023-10-09T11:20:15.688562Z","shell.execute_reply.started":"2023-10-09T11:20:12.282545Z"},"id":"wdzIm7wnFFhP","outputId":"a608f963-3cdb-431f-9587-c337508ed5e2","trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sample in features_val:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence\n","    #   (2) Prepend the `[CLS]` token to the start\n","    #   (3) Append the `[SEP]` token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        sample,                       # Sentence to encode\n","                        add_special_tokens = True,    # Add '[CLS]' and '[SEP]'\n","                        max_length = window_length,   # Pad & truncate all sentences\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True, # Construct attn. masks\n","                        return_tensors = 'pt',        # Return pytorch tensors\n","                   )\n","\n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_val)\n","\n","# Print sentence 0, now as a list of IDs\n","print('Original: ', features_val[0])\n","print('Tokenized: ', tokenizer.tokenize(features_val[0]))\n","print('Token IDs:', input_ids[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:15.691776Z","iopub.status.busy":"2023-10-09T11:20:15.691105Z","iopub.status.idle":"2023-10-09T11:20:15.697644Z","shell.execute_reply":"2023-10-09T11:20:15.696625Z","shell.execute_reply.started":"2023-10-09T11:20:15.691745Z"},"id":"tVvJ7tIFM7-_","trusted":true},"outputs":[],"source":["# Combine the validation inputs into a TensorDataset\n","val_dataset = TensorDataset(input_ids, attention_masks, labels)\n","\n","# Print the number of samples in the set\n","print('{:>5,} validation samples'.format(len(val_dataset)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:15.699799Z","iopub.status.busy":"2023-10-09T11:20:15.698779Z","iopub.status.idle":"2023-10-09T11:20:15.714363Z","shell.execute_reply":"2023-10-09T11:20:15.713580Z","shell.execute_reply.started":"2023-10-09T11:20:15.699768Z"},"trusted":true},"outputs":[],"source":["# Create an iterator for the dataset using the torch DataLoader class\n","\n","# For validation the order doesn't matter, so read them sequentially\n","validation_dataloader = DataLoader(\n","            val_dataset,                              # The validation samples\n","            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially\n","            batch_size = batch_size                   # Evaluate with this batch size\n","        )"]},{"cell_type":"markdown","metadata":{"id":"Vdw9BqKrOuYW"},"source":["### Train Classification Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:15.715842Z","iopub.status.busy":"2023-10-09T11:20:15.715532Z","iopub.status.idle":"2023-10-09T11:20:24.180085Z","shell.execute_reply":"2023-10-09T11:20:24.179193Z","shell.execute_reply.started":"2023-10-09T11:20:15.715813Z"},"id":"edhGTqsmOxjr","outputId":"f63c0862-67b1-498e-f270-52ed3e767b92","trusted":true},"outputs":[],"source":["# Load BertForSequenceClassification, the pretrained BERT model with a single\n","# linear classification layer on top\n","model = BertForSequenceClassification.from_pretrained(\n","    \"bert-base-uncased\",          # Use the 12-layer BERT model, with an uncased vocab\n","    num_labels = 2,               # The number of output labels, 2 for binary classification\n","    output_attentions = False,    # Whether the model returns attentions weights\n","    output_hidden_states = False, # Whether the model returns all hidden-states\n",")\n","\n","# Tell pytorch to run this model on the GPU\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:24.181535Z","iopub.status.busy":"2023-10-09T11:20:24.181217Z","iopub.status.idle":"2023-10-09T11:20:24.191875Z","shell.execute_reply":"2023-10-09T11:20:24.190947Z","shell.execute_reply.started":"2023-10-09T11:20:24.181512Z"},"id":"J5V6b1hNPYsb","outputId":"8ecc9b8b-83e9-4948-e64f-465beb218f00","trusted":true},"outputs":[],"source":["# Note: AdamW is a class from the huggingface library (as opposed to pytorch)\n","optimizer = AdamW(model.parameters(),\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5\n","                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8\n","                )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:24.194007Z","iopub.status.busy":"2023-10-09T11:20:24.193361Z","iopub.status.idle":"2023-10-09T11:20:24.200090Z","shell.execute_reply":"2023-10-09T11:20:24.199186Z","shell.execute_reply.started":"2023-10-09T11:20:24.193977Z"},"id":"c6cemYHRPb5C","trusted":true},"outputs":[],"source":["# Total number of training steps is [number of batches] x [number of epochs]\n","# (This is not the same as the number of training samples)\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler\n","scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:24.201574Z","iopub.status.busy":"2023-10-09T11:20:24.201124Z","iopub.status.idle":"2023-10-09T11:20:24.211626Z","shell.execute_reply":"2023-10-09T11:20:24.210613Z","shell.execute_reply.started":"2023-10-09T11:20:24.201541Z"},"id":"fRuxeVmbPpJ1","trusted":true},"outputs":[],"source":["# Helper function for calculating accuracy\n","\n","# Function to calculate the accuracy of the predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:24.213269Z","iopub.status.busy":"2023-10-09T11:20:24.212792Z","iopub.status.idle":"2023-10-09T11:20:24.226683Z","shell.execute_reply":"2023-10-09T11:20:24.225791Z","shell.execute_reply.started":"2023-10-09T11:20:24.213238Z"},"id":"RDn4EuLTPuMr","trusted":true},"outputs":[],"source":["# Helper function for formatting elapsed times as hh:mm:ss\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:20:24.228391Z","iopub.status.busy":"2023-10-09T11:20:24.228014Z","iopub.status.idle":"2023-10-09T11:38:05.020444Z","shell.execute_reply":"2023-10-09T11:38:05.019281Z","shell.execute_reply.started":"2023-10-09T11:20:24.228356Z"},"id":"7IujE0eqP2_S","outputId":"bddc2a8e-cb1a-4f42-b211-735bbfed4119","trusted":true},"outputs":[],"source":["random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","# We'll store a number of quantities such as training and validation loss,\n","# validation accuracy, and timings\n","training_stats = []\n","\n","# Measure the total training time for the whole run\n","total_t0 = time.time()\n","\n","# For each epoch...\n","for epoch_i in range(0, epochs):\n","\n","    # ========================================\n","    #               Training\n","    # ========================================\n","\n","    # Perform one full pass over the training set\n","\n","    print(\"\")\n","    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","    print('Training...')\n","\n","    # Measure how long the training epoch takes\n","    t0 = time.time()\n","\n","    # Reset the total loss for this epoch\n","    total_train_loss = 0\n","\n","    # Put the model into training mode. Don't be mislead--the call to\n","    # `train` just changes the *mode*, it doesn't *perform* the training.\n","    # `dropout` and `batchnorm` layers behave differently during training\n","    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n","    model.train()\n","\n","    # For each batch of training data...\n","    for step, batch in enumerate(train_dataloader):\n","\n","        # Progress update every 40 batches\n","        if step % 40 == 0 and not step == 0:\n","            # Calculate elapsed time in minutes\n","            elapsed = format_time(time.time() - t0)\n","\n","            # Report progress\n","            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","        # Unpack this training batch from the dataloader\n","        #\n","        # Also copy each tensor to the GPU using the\n","        # `to` method\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","\n","        # Always clear any previously calculated gradients before performing a\n","        # backward pass. PyTorch doesn't do this automatically because\n","        # accumulating the gradients is \"convenient while training RNNs\".\n","        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n","        model.zero_grad()\n","\n","        # Perform a forward pass (evaluate the model on this training batch).\n","        # The documentation for this `model` function is here:\n","        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","        # It returns different numbers of parameters depending on what arguments\n","        # arge given and what flags are set. For the useage here, it returns\n","        # the loss (because labels are provided) and the \"logits\"--the model\n","        # outputs prior to activation\n","\n","        result = model(b_input_ids,\n","                       token_type_ids=None,\n","                       attention_mask=b_input_mask,\n","                       labels=b_labels,\n","                       return_dict=True)\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the training loss over all of the batches so that it's possible to\n","        # calculate the average loss at the end. `loss` is a Tensor containing a\n","        # single value; the `.item()` function just returns the Python value\n","        # from the tensor\n","        total_train_loss += loss.item()\n","\n","        # Perform a backward pass to calculate the gradients\n","        loss.backward()\n","\n","        # Clip the norm of the gradients to 1.0.\n","        # This is to help prevent the \"exploding gradients\" problem\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","        # Update parameters and take a step using the computed gradient.\n","        # The optimizer dictates the \"update rule\"--how the parameters are\n","        # modified based on their gradients, the learning rate, etc\n","        optimizer.step()\n","\n","        # Update the learning rate\n","        scheduler.step()\n","\n","    # Calculate the average loss over all of the batches\n","    avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","    # Measure how long this epoch took\n","    training_time = format_time(time.time() - t0)\n","\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","    print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","    # ========================================\n","    #               Validation\n","    # ========================================\n","    # After the completion of each training epoch, measure the performance on\n","    # the validation set\n","\n","    print(\"\")\n","    print(\"Running Validation...\")\n","\n","    t0 = time.time()\n","\n","    # Put the model in evaluation mode--the dropout layers behave differently\n","    # during evaluation\n","    model.eval()\n","\n","    # Tracking variables\n","    total_eval_accuracy = 0\n","    total_eval_loss = 0\n","    nb_eval_steps = 0\n","    best_eval_loss = 1\n","\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","\n","        # Unpack this training batch from our dataloader\n","        #\n","        # Also copy each tensor to the GPU using\n","        # the `to` method\n","        #\n","        # `batch` contains three pytorch tensors:\n","        #   [0]: input ids\n","        #   [1]: attention masks\n","        #   [2]: labels\n","        b_input_ids = batch[0].to(device)\n","        b_input_mask = batch[1].to(device)\n","        b_labels = batch[2].to(device)\n","\n","        # Tell pytorch not to bother with constructing the compute graph during\n","        # the forward pass, since this is only needed for backprop (training)\n","        with torch.no_grad():\n","\n","            # Forward pass, calculate logit predictions.\n","            # token_type_ids is the same as the \"segment ids\", which\n","            # differentiates sentence 1 and 2 in 2-sentence tasks.\n","            # The documentation for this `model` function is here:\n","            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n","            # Get the \"logits\" output by the model. The \"logits\" are the output\n","            # values prior to applying an activation function like the softmax\n","            result = model(b_input_ids,\n","                           token_type_ids=None,\n","                           attention_mask=b_input_mask,\n","                           labels=b_labels,\n","                           return_dict=True)\n","\n","\n","        loss = result.loss\n","        logits = result.logits\n","\n","        # Accumulate the validation loss\n","        total_eval_loss += loss.item()\n","\n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # Calculate the accuracy for this batch of test sentences, and\n","        # accumulate it over all batches\n","        total_eval_accuracy += flat_accuracy(logits, label_ids)\n","\n","\n","    # Report the final accuracy for this validation run\n","    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","    # Calculate the average loss over all of the batches\n","    avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","    # Measure how long the validation run took\n","    validation_time = format_time(time.time() - t0)\n","\n","    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","    print(\"  Validation took: {:}\".format(validation_time))\n","    \n","    \"\"\"if avg_val_accuracy > best_eval_accuracy:\n","        torch.save(model, 'bert_model')\n","        best_eval_accuracy = avg_val_accuracy\"\"\"\n","    if avg_val_loss < best_eval_loss:\n","        torch.save(model, 'bert_model')\n","        best_eval_loss = avg_val_loss\n","\n","    # Record all statistics from this epoch\n","    training_stats.append(\n","        {\n","            'epoch': epoch_i + 1,\n","            'Training Loss': avg_train_loss,\n","            'Valid. Loss': avg_val_loss,\n","            'Valid. Accur.': avg_val_accuracy,\n","            'Training Time': training_time,\n","            'Validation Time': validation_time\n","        }\n","    )\n","\n","print(\"\")\n","print(\"Training complete!\")\n","\n","print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"markdown","metadata":{},"source":["### Training evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:05.035527Z","iopub.status.busy":"2023-10-09T11:38:05.032797Z","iopub.status.idle":"2023-10-09T11:38:05.066176Z","shell.execute_reply":"2023-10-09T11:38:05.065384Z","shell.execute_reply.started":"2023-10-09T11:38:05.035485Z"},"id":"6O_NbXFGMukX","outputId":"598ee076-134a-4a97-8079-b0e353a85a97","trusted":true},"outputs":[],"source":["# Summary of the training process\n","\n","# Display floats with two decimal places\n","\n","pd.reset_option('^display.')\n","pd.set_option('display.precision', 2)\n","\n","# Create a DataFrame from the training statistics\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table\n","df_stats"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:05.068350Z","iopub.status.busy":"2023-10-09T11:38:05.067687Z","iopub.status.idle":"2023-10-09T11:38:05.366175Z","shell.execute_reply":"2023-10-09T11:38:05.365397Z","shell.execute_reply.started":"2023-10-09T11:38:05.068318Z"},"id":"oBXl9wi2X8Sw","outputId":"d1f0f1ea-cbdc-4e4e-aca2-0202ab100b72","trusted":true},"outputs":[],"source":["# Use plot styling from seaborn\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"N5qrPf-NXQmV"},"source":["### Performance On Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:05.368328Z","iopub.status.busy":"2023-10-09T11:38:05.367470Z","iopub.status.idle":"2023-10-09T11:38:05.769149Z","shell.execute_reply":"2023-10-09T11:38:05.768134Z","shell.execute_reply.started":"2023-10-09T11:38:05.368297Z"},"trusted":true},"outputs":[],"source":["# Load best model\n","model = torch.load('bert_model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:05.776226Z","iopub.status.busy":"2023-10-09T11:38:05.773968Z","iopub.status.idle":"2023-10-09T11:38:07.975165Z","shell.execute_reply":"2023-10-09T11:38:07.974263Z","shell.execute_reply.started":"2023-10-09T11:38:05.776175Z"},"id":"F_6lRbSuXZYk","outputId":"80eafb6d-e281-4bf0-98f2-cbe1c33b3be9","trusted":true},"outputs":[],"source":["# Tokenize all of the sentences and map the tokens to thier word IDs.\n","input_ids = []\n","attention_masks = []\n","\n","# For every sentence...\n","for sample in features_test:\n","    # `encode_plus` will:\n","    #   (1) Tokenize the sentence\n","    #   (2) Prepend the `[CLS]` token to the start\n","    #   (3) Append the `[SEP]` token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the sentence to `max_length`\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        sample,                         # Sentence to encode\n","                        add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","                        max_length = window_length,     # Pad & truncate all sentences\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks\n","                        return_tensors = 'pt',          # Return pytorch tensors\n","                   )\n","\n","    # Add the encoded sentence to the list\n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim=0)\n","attention_masks = torch.cat(attention_masks, dim=0)\n","labels = torch.tensor(labels_test)\n","\n","# Create the DataLoader\n","prediction_data = TensorDataset(input_ids, attention_masks, labels)\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:07.977394Z","iopub.status.busy":"2023-10-09T11:38:07.976836Z","iopub.status.idle":"2023-10-09T11:38:18.112248Z","shell.execute_reply":"2023-10-09T11:38:18.111196Z","shell.execute_reply.started":"2023-10-09T11:38:07.977364Z"},"id":"YLak2zd_YYZi","outputId":"12cb3821-2b6f-4a58-cf8b-c456d4989a3f","trusted":true},"outputs":[],"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables\n","predictions , true_labels = [], []\n","\n","# Predict\n","for batch in prediction_dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","\n","    # Unpack the inputs from the dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","\n","    # Telling the model not to compute or store gradients, saving memory and\n","    # speeding up prediction\n","    with torch.no_grad():\n","      # Forward pass, calculate logit predictions\n","      result = model(b_input_ids,\n","                     token_type_ids=None,\n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","    logits = result.logits\n","\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()\n","\n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","print('    DONE.')\n","\n","print('\\nPositive samples: %d of %d (%.2f%%)' % (labels.sum().item(), len(labels), (labels.sum().item() / len(labels) * 100.0)))"]},{"cell_type":"markdown","metadata":{},"source":["### Test evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:18.114543Z","iopub.status.busy":"2023-10-09T11:38:18.113582Z","iopub.status.idle":"2023-10-09T11:38:18.133325Z","shell.execute_reply":"2023-10-09T11:38:18.132230Z","shell.execute_reply.started":"2023-10-09T11:38:18.114508Z"},"trusted":true},"outputs":[],"source":["# Flatten the true labels and predictions\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","flat_predictions_avg = []\n","# Adjust predictions\n","for i in range(len(true_labels)):\n","\n","    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n","    # and one column for \"1\"). Pick the label with the highest value and turn this\n","    # in to a list of 0s and 1s\n","    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","    flat_predictions_avg.append(pred_labels_i)\n","    \n","flat_predictions_avg = np.concatenate(flat_predictions_avg, axis=0)\n","\n","# Calculate additional performance indices\n","precision = precision_score(flat_true_labels, flat_predictions_avg)\n","recall = recall_score(flat_true_labels, flat_predictions_avg)\n","f1 = f1_score(flat_true_labels, flat_predictions_avg)\n","accuracy = accuracy_score(flat_true_labels, flat_predictions_avg)\n","\n","print('Accuracy: %.3f' % accuracy)\n","print('Precision: %.3f' % precision)\n","print('Recall: %.3f' % recall)\n","print('F1 Score: %.3f' % f1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:18.135549Z","iopub.status.busy":"2023-10-09T11:38:18.134906Z","iopub.status.idle":"2023-10-09T11:38:18.325324Z","shell.execute_reply":"2023-10-09T11:38:18.324509Z","shell.execute_reply.started":"2023-10-09T11:38:18.135512Z"},"trusted":true},"outputs":[],"source":["# Calculate the confusion matrix\n","cm = confusion_matrix(flat_true_labels, flat_predictions_avg)\n","\n","# Display the confusion matrix as a heatmap\n","plt.figure(figsize=(3, 3))\n","sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n","plt.title(\"Confusion Matrix\")\n","plt.xlabel(\"Predicted Labels\")\n","plt.ylabel(\"True Labels\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:18.327383Z","iopub.status.busy":"2023-10-09T11:38:18.326534Z","iopub.status.idle":"2023-10-09T11:38:20.270896Z","shell.execute_reply":"2023-10-09T11:38:20.270057Z","shell.execute_reply.started":"2023-10-09T11:38:18.327351Z"},"id":"cMysz7LeZlXm","outputId":"d989d0ba-519f-473f-c5ca-2badb95fac51","trusted":true},"outputs":[],"source":["# Use MCC here because the classes are imbalanced\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","\n","    # The predictions for this batch are a 2-column ndarray (one column for \"0\"\n","    # and one column for \"1\"). Pick the label with the highest value and turn this\n","    # in to a list of 0s and 1s\n","    pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","\n","    # Calculate and store the coef for this batch\n","    matthews = matthews_corrcoef(true_labels[i], pred_labels_i)\n","    matthews_set.append(matthews)\n","\n","# Create a barplot showing the MCC score for each batch of test samples\n","ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, errorbar=None)\n","\n","plt.title('MCC Score per Batch')\n","plt.ylabel('MCC Score (-1 to +1)')\n","plt.xlabel('Batch #')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:20.272869Z","iopub.status.busy":"2023-10-09T11:38:20.272070Z","iopub.status.idle":"2023-10-09T11:38:20.282948Z","shell.execute_reply":"2023-10-09T11:38:20.281965Z","shell.execute_reply.started":"2023-10-09T11:38:20.272832Z"},"id":"p09a5qmFZwI4","outputId":"e031628a-a552-4dcd-ae6b-a0070bfc4f64","trusted":true},"outputs":[],"source":["# Combine the results across all batches\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:20.285126Z","iopub.status.busy":"2023-10-09T11:38:20.284228Z","iopub.status.idle":"2023-10-09T11:38:20.294456Z","shell.execute_reply":"2023-10-09T11:38:20.293547Z","shell.execute_reply.started":"2023-10-09T11:38:20.285094Z"},"trusted":true},"outputs":[],"source":["# Combine the results across all batches\n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# Calculate the probabilities for the positive class\n","probs = flat_predictions[:, 1]\n","\n","# Calculate the false positive rate (fpr), true positive rate (tpr), and threshold values using the roc_curve function\n","fpr, tpr, thresholds = roc_curve(flat_true_labels, probs)\n","\n","# Calculate the Area Under the Curve (AUC) using the auc function\n","roc_auc = auc(fpr, tpr)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:38:20.296384Z","iopub.status.busy":"2023-10-09T11:38:20.295754Z","iopub.status.idle":"2023-10-09T11:38:20.594497Z","shell.execute_reply":"2023-10-09T11:38:20.593664Z","shell.execute_reply.started":"2023-10-09T11:38:20.296355Z"},"trusted":true},"outputs":[],"source":["# Plot the ROC curve\n","plt.rcParams[\"figure.figsize\"] = (8,5)\n","plt.figure()\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic')\n","plt.legend(loc=\"lower right\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["### Inspect prediction"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:40:26.200795Z","iopub.status.busy":"2023-10-09T11:40:26.200461Z","iopub.status.idle":"2023-10-09T11:40:26.216596Z","shell.execute_reply":"2023-10-09T11:40:26.215496Z","shell.execute_reply.started":"2023-10-09T11:40:26.200769Z"},"trusted":true},"outputs":[],"source":["predtiction_sample = []\n","\n","for index in range(0,len(flat_true_labels)):\n","    predtiction_sample.append(\n","        {\n","            'processedContent': features_test[index],\n","            'True label': flat_true_labels[index],\n","            'Predicted label': flat_predictions_avg[index]\n","        }\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:40:26.219129Z","iopub.status.busy":"2023-10-09T11:40:26.218242Z","iopub.status.idle":"2023-10-09T11:40:26.496923Z","shell.execute_reply":"2023-10-09T11:40:26.495718Z","shell.execute_reply.started":"2023-10-09T11:40:26.219097Z"},"trusted":true},"outputs":[],"source":["# Create a DataFrame from the statistics\n","df_stats = pd.DataFrame(data=predtiction_sample)\n","\n","# Create a styler object\n","styler = df_stats.style.set_table_styles([\n","    {'selector': 'th', 'props': [('text-align', 'left')]},\n","    {'selector': 'td', 'props': [('text-align', 'left')]}\n","])\n","\n","# Display the table with aligned content\n","styler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:42:37.760000Z","iopub.status.busy":"2023-10-09T11:42:37.759663Z","iopub.status.idle":"2023-10-09T11:42:37.770314Z","shell.execute_reply":"2023-10-09T11:42:37.769357Z","shell.execute_reply.started":"2023-10-09T11:42:37.759973Z"},"trusted":true},"outputs":[],"source":["# For a single entry\n","index = 0\n","pd.set_option('display.max_colwidth', None)  # Display full content in each cell\n","\n","predtiction_sample = []\n","predtiction_sample.append(\n","        {\n","            'Sample': features_test[index],\n","            'True label': flat_true_labels[index],\n","            'Predicted label': flat_predictions_avg[index]\n","        }\n","    )\n","\n","# Create a DataFrame from the statistics\n","df_stats = pd.DataFrame(data=predtiction_sample)\n","\n","# Create a styler object\n","styler = df_stats.style.set_table_styles([\n","    {'selector': 'th', 'props': [('text-align', 'left')]},\n","    {'selector': 'td', 'props': [('text-align', 'left')]}\n","])\n","\n","# Display the table with aligned content\n","styler"]},{"cell_type":"markdown","metadata":{},"source":["### Saving & Loading Fine-Tuned Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:44:04.229756Z","iopub.status.busy":"2023-10-09T11:44:04.229235Z","iopub.status.idle":"2023-10-09T11:44:05.021699Z","shell.execute_reply":"2023-10-09T11:44:05.020635Z","shell.execute_reply.started":"2023-10-09T11:44:04.229726Z"},"trusted":true},"outputs":[],"source":["# Save files\n","output_dir = \"/home/anon/working/Sentence_classifier_BERT\"\n","os.makedirs(output_dir, exist_ok=True)\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","tokenizer.save_pretrained(output_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:44:05.024147Z","iopub.status.busy":"2023-10-09T11:44:05.023585Z","iopub.status.idle":"2023-10-09T11:44:28.170101Z","shell.execute_reply":"2023-10-09T11:44:28.169163Z","shell.execute_reply.started":"2023-10-09T11:44:05.024114Z"},"trusted":true},"outputs":[],"source":["# Create a zip file\n","output_dir = '/home/anon/working/Sentence_classifier_BERT'\n","zip_path = '/home/anon/working/Sentence_classifier_BERT.zip'\n","\n","with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","    for root, _, files in os.walk(output_dir):\n","        for file in files:\n","            zipf.write(os.path.join(root, file), os.path.relpath(os.path.join(root, file), output_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:44:28.172229Z","iopub.status.busy":"2023-10-09T11:44:28.171660Z","iopub.status.idle":"2023-10-09T11:44:33.239902Z","shell.execute_reply":"2023-10-09T11:44:33.238957Z","shell.execute_reply.started":"2023-10-09T11:44:28.172183Z"},"trusted":true},"outputs":[],"source":["# Load a trained model and vocabulary that you have fine-tuned\n","zip_path = '/home/anon/working/Sentence_classifier_BERT.zip'\n","output_dir = '/home/anon/working/Sentence_classifier_BERT'\n","\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(output_dir)\n","\n","# Load the tokenizer and model\n","tokenizer = BertTokenizer.from_pretrained(output_dir)\n","\n","# Load the fine-tuned model\n","model = BertForSequenceClassification.from_pretrained(output_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Classification of unseen sample (an example)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-09T11:44:33.242775Z","iopub.status.busy":"2023-10-09T11:44:33.242057Z","iopub.status.idle":"2023-10-09T11:44:33.268444Z","shell.execute_reply":"2023-10-09T11:44:33.267413Z","shell.execute_reply.started":"2023-10-09T11:44:33.242737Z"},"trusted":true},"outputs":[],"source":["txt = \"random phrase threat kind sql injection other random word confuse classificator wonder result prediction\"           \n","# Apply tokenization as for the training part\n","inputs = tokenizer.encode_plus(\n","    txt,\n","    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n","    max_length = window_length,     # Pad & truncate all sentences\n","    pad_to_max_length = True,\n","    return_attention_mask = True,   # Construct attn. masks\n","    return_tensors = 'pt',          # Return pytorch tensors\n",")\n","\n","inputs = {key: value.to(device) for key, value in inputs.items()}\n","\n","# Make the prediction\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","    logits = outputs.logits\n","    \n","probabilities = torch.softmax(logits, dim=1)\n","predicted_class = torch.argmax(probabilities, dim=1).item()\n","\n","# Print the result\n","pd.set_option('display.max_colwidth', None)  # Display full content in each cell\n","\n","predtiction_sample = []\n","predtiction_sample.append(\n","        {\n","            'Sample': txt,\n","            'Predicted label': predicted_class\n","        }\n","    )\n","\n","# Create a DataFrame from the statistics\n","df_stats = pd.DataFrame(data=predtiction_sample)\n","\n","# Create a styler object\n","styler = df_stats.style.set_table_styles([\n","    {'selector': 'th', 'props': [('text-align', 'left')]},\n","    {'selector': 'td', 'props': [('text-align', 'left')]}\n","])\n","\n","# Display the table with aligned content\n","styler"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
