{"cells":[{"cell_type":"markdown","metadata":{},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-10-29T17:52:09.491419Z","iopub.status.busy":"2023-10-29T17:52:09.491095Z","iopub.status.idle":"2023-10-29T17:52:23.032799Z","shell.execute_reply":"2023-10-29T17:52:23.031976Z","shell.execute_reply.started":"2023-10-29T17:52:09.491391Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import time\n","import json\n","import torch\n","import zipfile\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from torch import cuda\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import RobertaTokenizerFast, RobertaForTokenClassification\n","from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:23.034970Z","iopub.status.busy":"2023-10-29T17:52:23.034472Z","iopub.status.idle":"2023-10-29T17:52:23.063106Z","shell.execute_reply":"2023-10-29T17:52:23.062218Z","shell.execute_reply.started":"2023-10-29T17:52:23.034942Z"},"trusted":true},"outputs":[],"source":["# Set the default device to GPU\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{},"source":["### Setup the pathes"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:23.064379Z","iopub.status.busy":"2023-10-29T17:52:23.064128Z","iopub.status.idle":"2023-10-29T17:52:23.074084Z","shell.execute_reply":"2023-10-29T17:52:23.073233Z","shell.execute_reply.started":"2023-10-29T17:52:23.064357Z"},"trusted":true},"outputs":[],"source":["model_input_dir = '/home/anon/input/ner-models/HF_NER_model'  # Directory with the fine-tuned RoBERTa model\n","\n","df_input_dir = '/home/anon/input/chunks-nlp/HF_chunks_NLP'  # Directory with the posts to label\n","json_output_dir = '/home/anon/working/outputs'  # Direcotry with the final predicted posts\n","if not os.path.exists(json_output_dir):\n","    os.makedirs(json_output_dir)\n","\n","intermediate_predictions_dir = '/home/anon/working/intermediate_predictions'  # Directory with intermediate predictions\n","if not os.path.exists(intermediate_predictions_dir):\n","    os.makedirs(intermediate_predictions_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Load the model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:23.076455Z","iopub.status.busy":"2023-10-29T17:52:23.076193Z","iopub.status.idle":"2023-10-29T17:52:32.680997Z","shell.execute_reply":"2023-10-29T17:52:32.680131Z","shell.execute_reply.started":"2023-10-29T17:52:23.076432Z"},"trusted":true},"outputs":[],"source":["# Load the tokenizer\n","tokenizer = RobertaTokenizerFast.from_pretrained(model_input_dir)\n","\n","# Load the fine-tuned model\n","model = RobertaForTokenClassification.from_pretrained(model_input_dir)\n","\n","# Copy the model to the GPU.\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.682465Z","iopub.status.busy":"2023-10-29T17:52:32.682217Z","iopub.status.idle":"2023-10-29T17:52:32.686685Z","shell.execute_reply":"2023-10-29T17:52:32.685666Z","shell.execute_reply.started":"2023-10-29T17:52:32.682443Z"},"trusted":true},"outputs":[],"source":["# Define the maximum sequence length for each window - as in the training phase\n","max_length = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.688542Z","iopub.status.busy":"2023-10-29T17:52:32.688204Z","iopub.status.idle":"2023-10-29T17:52:32.698749Z","shell.execute_reply":"2023-10-29T17:52:32.697884Z","shell.execute_reply.started":"2023-10-29T17:52:32.688511Z"},"trusted":true},"outputs":[],"source":["# Create list of categories\n","tags = [\"APT\", \"SECTEAM\", \"IDTY\", \"OS\", \"EMAIL\", \"LOC\", \"TIME\", \"IP\", \"DOM\", \"URL\", \"PROT\", \n","         \"FILE\", \"TOOL\", \"MD5\", \"SHA1\", \"SHA2\", \"MAL\", \"ENCR\", \"VULNAME\", \"VULID\", \"ACT\"]\n","limits = [\"B\", \"I\"]  # Annotation scheme\n","entity_tags = [\"O\"] + [limit + \"-\" + tag for tag in tags for limit in limits]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.699890Z","iopub.status.busy":"2023-10-29T17:52:32.699622Z","iopub.status.idle":"2023-10-29T17:52:32.709231Z","shell.execute_reply":"2023-10-29T17:52:32.708453Z","shell.execute_reply.started":"2023-10-29T17:52:32.699866Z"},"trusted":true},"outputs":[],"source":["# Create dictionaries\n","labels_to_ids = {label: str(i) for i,label in enumerate(entity_tags)}  # Maps individual tags to indices\n","ids_to_labels = {str(i): label for i,label in enumerate(entity_tags)}  # Maps indices to individual tags"]},{"cell_type":"markdown","metadata":{},"source":["### Functions definition"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.710981Z","iopub.status.busy":"2023-10-29T17:52:32.710688Z","iopub.status.idle":"2023-10-29T17:52:32.725166Z","shell.execute_reply":"2023-10-29T17:52:32.724207Z","shell.execute_reply.started":"2023-10-29T17:52:32.710958Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Load the dataset and setup values to start predictions\n","    Parameters: name of the file to retrieve\n","    Returns: the created dataframe, boundaries to start fetching for predictions\n","\"\"\"\n","\n","def load_dataset(file_name):\n","    # Retrieve pathes\n","    input_filename = file_name + '_unrelatedContentRemoval_iocDefanging_passiveActiveConversion_pronounsSubjectEllipsisResolution_stopwordsRemoval_internetSlangRemoval_aliasesHandling.json'\n","    file_path = os.path.join(df_input_dir, input_filename)\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name) + '.txt'\n","    \n","    with open(file_path, 'r', encoding='utf-8') as file:\n","            data = json.load(file)\n","            \n","    for i, entry in enumerate(data):\n","        sentence_list = []\n","        for sentence in entry['content']:\n","            if sentence == \"\":\n","                continue\n","            sentence_list.append(sentence)\n","        data[i]['content'] = sentence_list    \n","            \n","    df = pd.DataFrame(data)\n","    # Convert the 'date' column to datetime format\n","    df['date'] = pd.to_datetime(df['date'])\n","    # Format the 'date' column as MM-DD-YYYY\n","    df['date'] = df['date'].dt.strftime('%m-%d-%Y')\n","\n","    # Report the number of articles\n","    print('Number of posts: {:,}'.format(df.shape[0]))\n","    \n","    new_data = []\n","    for idx, row in df.iterrows():\n","        r_ID = row['ID']\n","        r_postID = row['postID']\n","        r_threadID = row['threadID']\n","        r_threadTitle = row['threadTitle']\n","        r_subforumID = row['subforumID']\n","        r_subforumTitle = row['subforumTitle']\n","        r_authorName = row['authorName']\n","        r_date = row['date']\n","        r_flatContent = row['flatContent']\n","        r_origin = row['origin']\n","        r_CTIrelevant = row['CTIrelevant']\n","        r_content = row['content']\n","        r_aliases_list = row['aliases_list']\n","        for cont, al in zip(r_content, r_aliases_list):\n","            new_data.append({\n","                'ID':r_ID,\n","                'postID':r_postID,\n","                'threadID':r_threadID,\n","                'threadTitle':r_threadTitle,\n","                'subforumID':r_subforumID,\n","                'subforumTitle':r_subforumTitle,\n","                'authorName':r_authorName,\n","                'date':r_date,\n","                'flatContent':r_flatContent,\n","                'origin':r_origin,\n","                'CTIrelevant':r_CTIrelevant,\n","                'content':cont,\n","                'aliases_list':al\n","            })\n","            \n","    df = pd.DataFrame(new_data)\n","\n","    # Report the number of sentences\n","    print('Number of sentences: {:,}'.format(df.shape[0]))\n","    \n","    # Read file with intermediate predictions, count the number of rows and set min_retrieved_data\n","    max_retrieved_data = df.shape[0]  \n","    min_retrieved_data = 0\n","\n","    if os.path.isfile(intermediate_predictions_path):\n","        with open(intermediate_predictions_path, 'r') as file:\n","            min_retrieved_data = sum(1 for _ in file)\n","    print(\"Resume prediction from row #\", min_retrieved_data)\n","    \n","    return df, max_retrieved_data, min_retrieved_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.727368Z","iopub.status.busy":"2023-10-29T17:52:32.727030Z","iopub.status.idle":"2023-10-29T17:52:32.740417Z","shell.execute_reply":"2023-10-29T17:52:32.739483Z","shell.execute_reply.started":"2023-10-29T17:52:32.727337Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Define the steps to tokenize the input and perform classification\n","    Parameters: name of the file to retrieve, dataframe on which operating, boundaries to start fetching for predictions\n","    Returns: Nothing -> Predictions are stored in the file intermediate_predictions_path\n","\"\"\"\n","def make_extraction(file_name, df, max_retrieved_data, min_retrieved_data):    \n","    # Retrieve path\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name) + '.txt'\n","\n","    if min_retrieved_data >= max_retrieved_data:\n","            print(\"Entity extraction already completed\")\n","            return\n","\n","    df_slice = df.loc[min_retrieved_data:max_retrieved_data]\n","    len_df = len(df_slice)\n","\n","    with open(intermediate_predictions_path, 'a') as file:\n","        for idx, row in tqdm(df_slice.iterrows(), total=len_df, desc='Inference'):\n","            sentence = row['content'].lower()\n","            \n","            inputs = tokenizer(sentence.strip().split(),\n","                                is_split_into_words=True,\n","                                return_offsets_mapping=True,\n","                                padding='max_length',\n","                                truncation=True,\n","                                max_length=max_length,\n","                                return_tensors=\"pt\")\n","\n","            # move to gpu\n","            ids = inputs[\"input_ids\"].to(device)\n","            mask = inputs[\"attention_mask\"].to(device)\n","\n","            # forward pass\n","            outputs = model(ids, attention_mask=mask)\n","            logits = outputs[0]\n","\n","            active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","            tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","            token_predictions = [ids_to_labels[str(i)] for i in flattened_predictions.cpu().numpy()]\n","            wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","            prediction = []\n","            for token_pred in wp_preds:\n","                if token_pred[0][0] == 'Ä ':  # Character identifying 1st token of a word\n","                    prediction.append(token_pred[1])\n","            padded_prediction = prediction + ['O'] * (len(sentence.split()) - len(prediction))  # Tokens padding in case of truncation\n","\n","            # Save intermediate predictions\n","            if idx > 0:\n","                file.write('\\n' + ','.join(padded_prediction))\n","            else:\n","                file.write(','.join(padded_prediction))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.743628Z","iopub.status.busy":"2023-10-29T17:52:32.743324Z","iopub.status.idle":"2023-10-29T17:52:32.753814Z","shell.execute_reply":"2023-10-29T17:52:32.752922Z","shell.execute_reply.started":"2023-10-29T17:52:32.743604Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","Show the statistics of the extracted entities and save them in a json file then release the dataframe\n","    Parameters: name of the file to retrieve, dataframe on which operating, upper boundary\n","    Returns: Nothing -> Entities are stored in the file intermediate_predictions_path\n","\"\"\"\n","def save_predictions(file_name, df, max_retrieved_data):\n","    # Retrieve pathes\n","    intermediate_predictions_path = os.path.join(intermediate_predictions_dir, file_name) + '.txt'\n","    json_output_path = os.path.join(json_output_dir, file_name) + '.json'\n","    \n","    # After all entities are predicted, retrieve all their tokens\n","    with open(intermediate_predictions_path, 'r') as file:\n","        lines = file.readlines()\n","    res = [row.strip() for row in lines]\n","\n","    # Create field for tags\n","    df['tags'] = pd.Series(res)\n","    \n","    # Group the DataFrame by 'ID', 'date', and 'origin' and concatenate the 'content' strings\n","    df = df.groupby(['ID', 'postID', 'threadID', 'threadTitle', 'subforumID', 'subforumTitle', 'authorName', 'date', 'flatContent', 'origin', 'CTIrelevant']).agg({'content': list, 'tags': list, 'aliases_list': list}).reset_index()\n","    \n","    # Save files\n","    print(\"Saving json to %s\\n\" % json_output_path)\n","\n","    # Save the DataFrame to a JSON file in the specified directory\n","    df.to_json(json_output_path, orient='records', indent=4)\n","    \n","    # Free resources\n","    del df"]},{"cell_type":"markdown","metadata":{},"source":["### Execution"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.755122Z","iopub.status.busy":"2023-10-29T17:52:32.754847Z","iopub.status.idle":"2023-10-29T17:52:32.766125Z","shell.execute_reply":"2023-10-29T17:52:32.765251Z","shell.execute_reply.started":"2023-10-29T17:52:32.755098Z"},"trusted":true},"outputs":[],"source":["def execute_code(filename):\n","    print(\"Retrieve\", filename)\n","    \n","    # Check if an output_json already exists for the given file -> if yes, skip to the next one\n","    json_output_path = os.path.join(json_output_dir, filename) + '.json'\n","    if os.path.isfile(json_output_path):\n","        print(\"File already processed, skipped\\n\")\n","        return\n","        \n","    # Load the dataset from df_input_dir\n","    df, max_retrieved_data, min_retrieved_data = load_dataset(filename)\n","    \n","    # Entity extraction\n","    make_extraction(filename, df, max_retrieved_data, min_retrieved_data)\n","    \n","    # Save predictions\n","    save_predictions(filename, df, max_retrieved_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.767685Z","iopub.status.busy":"2023-10-29T17:52:32.767222Z","iopub.status.idle":"2023-10-29T17:52:32.774620Z","shell.execute_reply":"2023-10-29T17:52:32.773760Z","shell.execute_reply.started":"2023-10-29T17:52:32.767659Z"},"trusted":true},"outputs":[],"source":["choose_interval = True  # True if one wants to execute a specified interval of chunks"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T17:52:32.775955Z","iopub.status.busy":"2023-10-29T17:52:32.775663Z","iopub.status.idle":"2023-10-29T20:29:02.774471Z","shell.execute_reply":"2023-10-29T20:29:02.773375Z","shell.execute_reply.started":"2023-10-29T17:52:32.775931Z"},"trusted":true},"outputs":[],"source":["if choose_interval:\n","    for i in range(1,168):  \n","        filename = 'HF_{0}'.format(i)\n","        execute_code(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T20:29:02.776316Z","iopub.status.busy":"2023-10-29T20:29:02.775942Z","iopub.status.idle":"2023-10-29T20:29:02.781107Z","shell.execute_reply":"2023-10-29T20:29:02.780049Z","shell.execute_reply.started":"2023-10-29T20:29:02.776283Z"},"trusted":true},"outputs":[],"source":["if not choose_interval:\n","    for filename in os.listdir(df_input_dir):\n","        execute_code(filename)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T20:29:02.782611Z","iopub.status.busy":"2023-10-29T20:29:02.782312Z","iopub.status.idle":"2023-10-29T20:29:03.538186Z","shell.execute_reply":"2023-10-29T20:29:03.537412Z","shell.execute_reply.started":"2023-10-29T20:29:02.782587Z"},"trusted":true},"outputs":[],"source":["# Directory path containing the intermediate files to zip\n","source_directory = intermediate_predictions_dir\n","\n","# Zip file path\n","zip_file_path = '/home/anon/working/intermediate_predictions.zip'\n","\n","# Create a ZIP archive\n","with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","    # Loop through all files in the directory and add them to the ZIP archive\n","    for root, _, files in os.walk(source_directory):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            zipf.write(file_path, os.path.relpath(file_path, source_directory))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-29T20:29:03.539539Z","iopub.status.busy":"2023-10-29T20:29:03.539260Z","iopub.status.idle":"2023-10-29T20:29:12.267420Z","shell.execute_reply":"2023-10-29T20:29:12.266636Z","shell.execute_reply.started":"2023-10-29T20:29:03.539514Z"},"trusted":true},"outputs":[],"source":["# Directory path containing the output files to zip\n","source_directory = json_output_dir\n","\n","# Zip file path\n","zip_file_path = '/home/anon/working/json_output.zip'\n","\n","# Create a ZIP archive\n","with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n","    # Loop through all files in the directory and add them to the ZIP archive\n","    for root, _, files in os.walk(source_directory):\n","        for file in files:\n","            file_path = os.path.join(root, file)\n","            zipf.write(file_path, os.path.relpath(file_path, source_directory))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
